{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"App Modernization Intro \u00b6 Welcome to the App Modernization Introduction workshop! In the workshop you will learn foundational container technologies for building modern, scalable, portable, and robust applications. You will learn about Containers, Kubernetes and OpenShift, and Helm. Pre-requirements \u00b6 IBM Cloud account, Docker Hub account, Access to Play with Docker using Docker account, App Modernization 101 \u00b6 Each topic in this workshop consists of a technical overview and a hands-on lab. SESSION 1 Welcome - Introduction All 10 mins Lecture: Overview of Docker Remko de Knikker 30 mins Lab: Docker 101 Remko de Knikker 60 mins SESSION 2 Lecture: Overview of Kubernetes Remko de Knikker 30 mins Lab: Kubernetes 101 Remko de Knikker 60 mins SESSION 3 Lecture: CI/CD for Microservices Remko de Knikker 30 mins Lecture: Helm Remko de Knikker 20 mins Lab: Helm 101 Remko de Knikker 60 mins Conclusion All SESSION EXTRA Lab: CI/CD with Jenkins 101 self-paced 60 mins Lab: Deploy Open Liberty App using Source-to-Image (S2I) self-paced 90 mins Technologies \u00b6 Docker Red Hat OpenShift Kubernetes Service (ROKS) 5.6 Helm Credits \u00b6 Remko De Knikker","title":"Workshop Overview"},{"location":"#app-modernization-intro","text":"Welcome to the App Modernization Introduction workshop! In the workshop you will learn foundational container technologies for building modern, scalable, portable, and robust applications. You will learn about Containers, Kubernetes and OpenShift, and Helm.","title":"App Modernization Intro"},{"location":"#pre-requirements","text":"IBM Cloud account, Docker Hub account, Access to Play with Docker using Docker account,","title":"Pre-requirements"},{"location":"#app-modernization-101","text":"Each topic in this workshop consists of a technical overview and a hands-on lab. SESSION 1 Welcome - Introduction All 10 mins Lecture: Overview of Docker Remko de Knikker 30 mins Lab: Docker 101 Remko de Knikker 60 mins SESSION 2 Lecture: Overview of Kubernetes Remko de Knikker 30 mins Lab: Kubernetes 101 Remko de Knikker 60 mins SESSION 3 Lecture: CI/CD for Microservices Remko de Knikker 30 mins Lecture: Helm Remko de Knikker 20 mins Lab: Helm 101 Remko de Knikker 60 mins Conclusion All SESSION EXTRA Lab: CI/CD with Jenkins 101 self-paced 60 mins Lab: Deploy Open Liberty App using Source-to-Image (S2I) self-paced 90 mins","title":"App Modernization 101"},{"location":"#technologies","text":"Docker Red Hat OpenShift Kubernetes Service (ROKS) 5.6 Helm","title":"Technologies"},{"location":"#credits","text":"Remko De Knikker","title":"Credits"},{"location":"generatedContent/","text":"This content is generated! Do not edit directly! Please run aggregate-labs.sh to repopulate with latest content from agenda.txt!","title":"Index"},{"location":"generatedContent/docker101/lab-1/","text":"Lab 1 - Running Your First Container \u00b6 Prerequisites \u00b6 Docker Hub account, Access to a client terminal with Docker daemon, Play with Docker or localhost. Overview \u00b6 In this lab, you will run your first Docker container. Containers are just a process (or a group of processes) running in isolation. Isolation is achieved via linux namespaces, control groups (cgroups), seccomp and SELinux. Note that linux namespaces and control groups are built into the linux kernel! Other than the linux kernel itself, there is nothing special about containers. What makes containers useful is the tooling that surrounds it. For these labs, we will be using Docker, which has been a widely adopted tool for using containers to build applications. Docker provides developers and operators with a friendly interface to build, ship and run containers on any environment with a Docker engine. Because Docker client requires a Docker engine, an alternative is to use Podman , which is a deamonless container engine to develop, manage and run OCI containers and is able to run containers as root or in rootless mode. For those reasons, we recommend Podman but because of adoption, this lab still uses Docker. The first part of this lab, we will run our first container, and learn how to inspect it. We will be able to witness the namespace isolation that we acquire from the linux kernel. After we run our first container, we will dive into other uses of containers. You can find many examples of these on the Docker Store, and we will run several different types of containers on the same host. This will allow us to see the benefit of isolation- where we can run multiple containers on the same host without conflicts. We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation . Get Started \u00b6 Run docker -h , $ docker -h Flag shorthand -h has been deprecated, please use --help Usage: docker [OPTIONS] COMMAND A self-sufficient runtime for containers ... Management Commands: builder Manage builds config Manage Docker configs container Manage containers engine Manage the docker engine image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker trust Manage trust on Docker images volume Manage volumes The Docker command line can be used to manage several features of the Docker Engine. In this lab, we will mainly focus on the container command. You can additionally review the version of your Docker installation, docker version Client: Version: 19 .03.6 ... Server: Docker Engine - Community Engine Version: 19 .03.5 ... You note that Docker installs both a Client and a Server: Docker Engine . Step 1: Run your first container \u00b6 We are going to use the Docker CLI to run our first container. Open a terminal on your local computer Run docker container run -t ubuntu top Use the docker container run command to run a container with the ubuntu image using the top command. The -t flags allocate a pseudo-TTY which we need for the top to work correctly. $ docker container run -it ubuntu top Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu aafe6b5e13de: Pull complete 0a2b43a72660: Pull complete 18bdd1e546d2: Pull complete 8198342c3e05: Pull complete f56970a44fd4: Pull complete Digest: sha256:f3a61450ae43896c4332bda5e78b453f4a93179045f20c8181043b26b5e79028 Status: Downloaded newer image for ubuntu:latest The docker run command will result first in a docker pull to download the ubuntu image onto your host. Once it is downloaded, it will start the container. The output for the running container should look like this: top - 20 :32:46 up 3 days, 17 :40, 0 users, load average: 0 .00, 0 .01, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .0 us, 0 .1 sy, 0 .0 ni, 99 .9 id, 0 .0 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046768 total, 173308 free, 117248 used, 1756212 buff/cache KiB Swap: 1048572 total, 1048572 free, 0 used. 1548356 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36636 3072 2640 R 0 .3 0 .2 0 :00.04 top top is a linux utility that prints the processes on a system and orders them by resource consumption. Notice that there is only a single process in this output: it is the top process itself. We don't see other processes from our host in this list because of the PID namespace isolation. Containers use linux namespaces to provide isolation of system resources from other containers or the host. The PID namespace provides isolation for process IDs. If you run top while inside the container, you will notice that it shows the processes within the PID namespace of the container, which is much different than what you can see if you ran top on the host. Even though we are using the ubuntu image, it is important to note that our container does not have its own kernel. Its uses the kernel of the host and the ubuntu image is used only to provide the file system and tools available on an ubuntu system. Inspect the container with docker container exec The docker container exec command is a way to \"enter\" a running container's namespaces with a new process. Open a new terminal. Using play-with-docker.com, to open a new terminal connected to node1, click \"Add New Instance\" on the lefthand side, then ssh from node2 into node1 using the IP that is listed by 'node1 '. For example: [node2] (local) root@192.168.0.17 ~ $ ssh 192 .168.0.18 [node1] (local) root@192.168.0.18 ~ $ In the new terminal, use the docker container ls command to get the ID of the running container you just created. $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b3ad2a23fab3 ubuntu \"top\" 29 minutes ago Up 29 minutes goofy_nobel Then use that id to run bash inside that container using the docker container exec command. Since we are using bash and want to interact with this container from our terminal, use -it flags to run using interactive mode while allocating a psuedo-terminal. $ docker container exec -it b3ad2a23fab3 bash root@b3ad2a23fab3:/# And Voila! We just used the docker container exec command to \"enter\" our container's namespaces with our bash process. Using docker container exec with bash is a common pattern to inspect a docker container. Notice the change in the prefix of your terminal. e.g. root@b3ad2a23fab3:/ . This is an indication that we are running bash \"inside\" of our container. Note : This is not the same as ssh'ing into a separate host or a VM. We don't need an ssh server to connect with a bash process. Remember that containers use kernel-level features to achieve isolation and that containers run on top of the kernel. Our container is just a group of processes running in isolation on the same host, and we can use docker container exec to enter that isolation with the bash process. After running docker container exec , the group of processes running in isolation (i.e. our container) include top and bash . From the same termina, run ps -ef to inspect the running processes. root@b3ad2a23fab3:/# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 20:34 ? 00:00:00 top root 17 0 0 21:06 ? 00:00:00 bash root 27 17 0 21:14 ? 00:00:00 ps -ef You should see only the top process, bash process and our ps process. For comparison, exit the container, and run ps -ef or top on the host. These commands will work on linux or mac. For windows, you can inspect the running processes using tasklist . root@b3ad2a23fab3:/# exit exit $ ps -ef # Lots of processes! Technical Deep Dive PID is just one of the linux namespaces that provides containers with isolation to system resources. Other linux namespaces include: - MNT - Mount and unmount directories without affecting other namespaces - NET - Containers have their own network stack - IPC - Isolated interprocess communication mechanisms such as message queues. - User - Isolated view of users on the system - UTC - Set hostname and domain name per container These namespaces together provide the isolation for containers that allow them to run together securely and without conflict with other containers running on the same system. Next, we will demonstrate different uses of containers. and the benefit of isolation as we run multiple containers on the same host. Note : Namespaces are a feature of the linux kernel. But Docker allows you to run containers on Windows and Mac... how does that work? The secret is that embedded in the Docker product or Docker engine is a linux subsystem. Docker open-sourced this linux subsystem to a new project: LinuxKit . Being able to run containers on many different platforms is one advantage of using the Docker tooling with containers. In addition to running linux containers on Windows using a linux subsystem, native Windows containers are now possible due the creation of container primitives on the Windows OS. Native Windows containers can be run on Windows 10 or Windows Server 2016 or newer. Note : if you run this exercise in a containerized terminal and execute the ps -ef command in the terminal, e.g. in https://labs.cognitiveclass.ai , you will still see a limited set of processes after exiting the exec command. You can try to run the ps -ef command in a terminal on your local machine to see all processes. Clean up the container running the top processes by typing: <ctrl>-c , list all containers and remove the containers by their id. docker ps -a docker rm <CONTAINER ID> Step 2: Run Multiple Containers \u00b6 Explore the Docker Hub The Docker Hub is the public central registry for Docker images, which contains community and official images. When searching for images you will find filters for \"Docker Certified\", \"Verified Publisher\" and \"Official Images\" images. Select the \"Docker Certified\" filter, to find images that are deemed enterprise-ready and are tested with Docker Enterprise Edition product. It is important to avoid using unverified content from the Docker Store when developing your own images that are intended to be deployed into the production environment. These unverified images may contain security vulnerabilities or possibly even malicious software. In Step 2 of this lab, we will start a couple of containers using some verified images from the Docker Hub: nginx web server, and mongo database. Run an Nginx server Let's run a container using the official Nginx image from the Docker Hub. $ docker container run --detach --publish 8080 :80 --name nginx nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 36a46ebd5019: Pull complete 57168433389f: Pull complete 332ec8285c50: Pull complete Digest: sha256:c15f1fb8fd55c60c72f940a76da76a5fccce2fefa0dd9b17967b9e40b0355316 Status: Downloaded newer image for nginx:latest 5e1bf0e6b926bd73a66f98b3cbe23d04189c16a43d55dd46b8486359f6fdf048 We are using a couple of new flags here. The --detach flag will run this container in the background. The publish flag publishes port 80 in the container (the default port for nginx), via port 8080 on our host. Remember that the NET namespace gives processes of the container their own network stack. The --publish flag is a feature that allows us to expose networking through the container onto the host. How do you know port 80 is the default port for nginx? Because it is listed in the documentation on the Docker Hub. In general, the documentation for the verified images is very good, and you will want to refer to them when running containers using those images. We are also specifying the --name flag, which names the container. Every container has a name, if you don't specify one, Docker will randomly assign one for you. Specifying your own name makes it easier to run subsequent commands on your container since you can reference the name instead of the id of the container. For example: docker container inspect nginx instead of docker container inspect 5e1 . Since this is the first time you are running the nginx container, it will pull down the nginx image from the Docker Store. Subsequent containers created from the Nginx image will use the existing image located on your host. Nginx is a lightweight web server. You can access it on port 8080 on your localhost. Access the nginx server on localhost:8080 . curl localhost:8080 will return the HTML home page of Nginx, <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> If you are using play-with-docker, look for the 8080 link near the top of the page, or if you run a Docker client with access to a local browser, Run a mongo DB server Now, run a mongoDB server. We will use the official mongoDB image from the Docker Hub. Instead of using the latest tag (which is the default if no tag is specified), we will use a specific version of the mongo image: 4.4. $ docker container run --detach --publish 8081 :27017 --name mongo mongo:4.4 Unable to find image mongo:4.4 locally 4 .4: Pulling from library/mongo d13d02fa248d: Already exists bc8e2652ce92: Pull complete 3cc856886986: Pull complete c319e9ec4517: Pull complete b4cbf8808f94: Pull complete cb98a53e6676: Pull complete f0485050cd8a: Pull complete ac36cdc414b3: Pull complete 61814e3c487b: Pull complete 523a9f1da6b9: Pull complete 3b4beaef77a2: Pull complete Digest: sha256:d13c897516e497e898c229e2467f4953314b63e48d4990d3215d876ef9d1fc7c Status: Downloaded newer image for mongo:4.4 d8f614a4969fb1229f538e171850512f10f490cb1a96fca27e4aa89ac082eba5 Again, since this is the first time we are running a mongo container, we will pull down the mongo image from the Docker Store. We are using the --publish flag to expose the 27017 mongo port on our host. We have to use a port other than 8080 for the host mapping, since that port is already exposed on our host. Again refer to the official docs on the Docker Hub to get more details about using the mongo image. Access localhost:8081 to see some output from mongo. curl localhost:8081 which will return a warning from MongoDB, It looks like you are trying to access MongoDB over HTTP on the native driver port. If you are using play-with-docker, look for the 8080 link near the top of the page. Check your running containers with docker container ls $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6777df89fea nginx \"nginx -g 'daemon ...\" Less than a second ago Up 2 seconds 0 .0.0.0:8080->80/tcp nginx ead80a0db505 mongo \"docker-entrypoint...\" 17 seconds ago Up 19 seconds 0 .0.0.0:8081->27017/tcp mongo af549dccd5cf ubuntu \"top\" 5 minutes ago Up 5 minutes priceless_kepler You should see that you have an Nginx web server container, and a MongoDB container running on your host. Note that we have not configured these containers to talk to each other. You can see the \"nginx\" and \"mongo\" names that we gave to our containers, and the random name (in my case \"priceless_kepler\") that was generated for the ubuntu container. You can also see that the port mappings that we specified with the --publish flag. For more details information on these running containers you can use the docker container inspect [container id command. One thing you might notice is that the mongo container is running the docker-entrypoint command. This is the name of the executable that is run when the container is started. The mongo image requires some prior configuration before kicking off the DB process. You can see exactly what the script does by looking at it on github . Typically, you can find the link to the github source from the image description page on the Docker Store website. Containers are self-contained and isolated, which means we can avoid potential conflicts between containers with different system or runtime dependencies. For example: deploying an app that uses Java 7 and another app that uses Java 8 on the same host. Or running multiple nginx containers that all have port 80 as their default listening ports (if exposing on the host using the --publish flag, the ports selected for the host will need to be unique). Isolation benefits are possible because of Linux Namespaces. Note : You didn't have to install anything on your host (other than Docker) to run these processes! Each container includes the dependencies that it needs within the container, so you don't need to install anything on your host directly. Running multiple containers on the same host gives us the ability to fully utilize the resources (cpu, memory, etc) available on single host. This can result in huge cost savings for an enterprise. While running images directly from the Docker Hub can be useful at times, it is more useful to create custom images, and refer to official images as the starting point for these images. We will dive into building our own custom images in Lab 2. Step 3: Clean Up \u00b6 Completing this lab results in a bunch of running containers on your host. Let's clean these up. First get a list of the containers running using docker container ls . $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6777df89fea nginx \"nginx -g 'daemon ...\" 3 minutes ago Up 3 minutes 0 .0.0.0:8080->80/tcp nginx ead80a0db505 mongo \"docker-entrypoint...\" 3 minutes ago Up 3 minutes 0 .0.0.0:8081->27017/tcp mongo af549dccd5cf ubuntu \"top\" 8 minutes ago Up 8 minutes priceless_kepler Next, run docker container stop [container id] for each container in the list. You can also use the names of the containers that you specified before. $ docker container stop d67 ead af5 d67 ead af5 Note : You only have to reference enough digits of the ID to be unique. Three digits is almost always enough. Remove the stopped containers docker system prune is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images. $ docker system prune WARNING! This will remove: - all stopped containers - all volumes not used by at least one container - all networks not used by at least one container - all dangling images Are you sure you want to continue ? [ y/N ] y Deleted Containers: 7872fd96ea4695795c41150a06067d605f69702dbcb9ce49492c9029f0e1b44b 60abd5ee65b1e2732ddc02b971a86e22de1c1c446dab165462a08b037ef7835c 31617fdd8e5f584c51ce182757e24a1c9620257027665c20be75aa3ab6591740 Total reclaimed space: 12B Summary \u00b6 In this lab, you created your first Ubuntu, Nginx and MongoDB containers. Key Takeaways Containers are composed of linux namespaces and control groups that provide isolation from other containers and the host. Because of the isolation properties of containers, you can schedule many containers on a single host without worrying about conflicting dependencies. This makes it easier to run multiple containers on a single host: fully utilizing resources allocated to that host, and ultimately saving some money on server costs. Avoid using unverified content from the Docker Store when developing your own images because these images may contain security vulnerabilities or possibly even malicious software. Containers include everything they need to run the processes within them, so there is no need to install additional dependencies directly on your host.","title":"Lab 1. Run your First Container"},{"location":"generatedContent/docker101/lab-1/#lab-1-running-your-first-container","text":"","title":"Lab 1 - Running Your First Container"},{"location":"generatedContent/docker101/lab-1/#prerequisites","text":"Docker Hub account, Access to a client terminal with Docker daemon, Play with Docker or localhost.","title":"Prerequisites"},{"location":"generatedContent/docker101/lab-1/#overview","text":"In this lab, you will run your first Docker container. Containers are just a process (or a group of processes) running in isolation. Isolation is achieved via linux namespaces, control groups (cgroups), seccomp and SELinux. Note that linux namespaces and control groups are built into the linux kernel! Other than the linux kernel itself, there is nothing special about containers. What makes containers useful is the tooling that surrounds it. For these labs, we will be using Docker, which has been a widely adopted tool for using containers to build applications. Docker provides developers and operators with a friendly interface to build, ship and run containers on any environment with a Docker engine. Because Docker client requires a Docker engine, an alternative is to use Podman , which is a deamonless container engine to develop, manage and run OCI containers and is able to run containers as root or in rootless mode. For those reasons, we recommend Podman but because of adoption, this lab still uses Docker. The first part of this lab, we will run our first container, and learn how to inspect it. We will be able to witness the namespace isolation that we acquire from the linux kernel. After we run our first container, we will dive into other uses of containers. You can find many examples of these on the Docker Store, and we will run several different types of containers on the same host. This will allow us to see the benefit of isolation- where we can run multiple containers on the same host without conflicts. We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation .","title":"Overview"},{"location":"generatedContent/docker101/lab-1/#get-started","text":"Run docker -h , $ docker -h Flag shorthand -h has been deprecated, please use --help Usage: docker [OPTIONS] COMMAND A self-sufficient runtime for containers ... Management Commands: builder Manage builds config Manage Docker configs container Manage containers engine Manage the docker engine image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker trust Manage trust on Docker images volume Manage volumes The Docker command line can be used to manage several features of the Docker Engine. In this lab, we will mainly focus on the container command. You can additionally review the version of your Docker installation, docker version Client: Version: 19 .03.6 ... Server: Docker Engine - Community Engine Version: 19 .03.5 ... You note that Docker installs both a Client and a Server: Docker Engine .","title":"Get Started"},{"location":"generatedContent/docker101/lab-1/#step-1-run-your-first-container","text":"We are going to use the Docker CLI to run our first container. Open a terminal on your local computer Run docker container run -t ubuntu top Use the docker container run command to run a container with the ubuntu image using the top command. The -t flags allocate a pseudo-TTY which we need for the top to work correctly. $ docker container run -it ubuntu top Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu aafe6b5e13de: Pull complete 0a2b43a72660: Pull complete 18bdd1e546d2: Pull complete 8198342c3e05: Pull complete f56970a44fd4: Pull complete Digest: sha256:f3a61450ae43896c4332bda5e78b453f4a93179045f20c8181043b26b5e79028 Status: Downloaded newer image for ubuntu:latest The docker run command will result first in a docker pull to download the ubuntu image onto your host. Once it is downloaded, it will start the container. The output for the running container should look like this: top - 20 :32:46 up 3 days, 17 :40, 0 users, load average: 0 .00, 0 .01, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .0 us, 0 .1 sy, 0 .0 ni, 99 .9 id, 0 .0 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046768 total, 173308 free, 117248 used, 1756212 buff/cache KiB Swap: 1048572 total, 1048572 free, 0 used. 1548356 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36636 3072 2640 R 0 .3 0 .2 0 :00.04 top top is a linux utility that prints the processes on a system and orders them by resource consumption. Notice that there is only a single process in this output: it is the top process itself. We don't see other processes from our host in this list because of the PID namespace isolation. Containers use linux namespaces to provide isolation of system resources from other containers or the host. The PID namespace provides isolation for process IDs. If you run top while inside the container, you will notice that it shows the processes within the PID namespace of the container, which is much different than what you can see if you ran top on the host. Even though we are using the ubuntu image, it is important to note that our container does not have its own kernel. Its uses the kernel of the host and the ubuntu image is used only to provide the file system and tools available on an ubuntu system. Inspect the container with docker container exec The docker container exec command is a way to \"enter\" a running container's namespaces with a new process. Open a new terminal. Using play-with-docker.com, to open a new terminal connected to node1, click \"Add New Instance\" on the lefthand side, then ssh from node2 into node1 using the IP that is listed by 'node1 '. For example: [node2] (local) root@192.168.0.17 ~ $ ssh 192 .168.0.18 [node1] (local) root@192.168.0.18 ~ $ In the new terminal, use the docker container ls command to get the ID of the running container you just created. $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b3ad2a23fab3 ubuntu \"top\" 29 minutes ago Up 29 minutes goofy_nobel Then use that id to run bash inside that container using the docker container exec command. Since we are using bash and want to interact with this container from our terminal, use -it flags to run using interactive mode while allocating a psuedo-terminal. $ docker container exec -it b3ad2a23fab3 bash root@b3ad2a23fab3:/# And Voila! We just used the docker container exec command to \"enter\" our container's namespaces with our bash process. Using docker container exec with bash is a common pattern to inspect a docker container. Notice the change in the prefix of your terminal. e.g. root@b3ad2a23fab3:/ . This is an indication that we are running bash \"inside\" of our container. Note : This is not the same as ssh'ing into a separate host or a VM. We don't need an ssh server to connect with a bash process. Remember that containers use kernel-level features to achieve isolation and that containers run on top of the kernel. Our container is just a group of processes running in isolation on the same host, and we can use docker container exec to enter that isolation with the bash process. After running docker container exec , the group of processes running in isolation (i.e. our container) include top and bash . From the same termina, run ps -ef to inspect the running processes. root@b3ad2a23fab3:/# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 20:34 ? 00:00:00 top root 17 0 0 21:06 ? 00:00:00 bash root 27 17 0 21:14 ? 00:00:00 ps -ef You should see only the top process, bash process and our ps process. For comparison, exit the container, and run ps -ef or top on the host. These commands will work on linux or mac. For windows, you can inspect the running processes using tasklist . root@b3ad2a23fab3:/# exit exit $ ps -ef # Lots of processes! Technical Deep Dive PID is just one of the linux namespaces that provides containers with isolation to system resources. Other linux namespaces include: - MNT - Mount and unmount directories without affecting other namespaces - NET - Containers have their own network stack - IPC - Isolated interprocess communication mechanisms such as message queues. - User - Isolated view of users on the system - UTC - Set hostname and domain name per container These namespaces together provide the isolation for containers that allow them to run together securely and without conflict with other containers running on the same system. Next, we will demonstrate different uses of containers. and the benefit of isolation as we run multiple containers on the same host. Note : Namespaces are a feature of the linux kernel. But Docker allows you to run containers on Windows and Mac... how does that work? The secret is that embedded in the Docker product or Docker engine is a linux subsystem. Docker open-sourced this linux subsystem to a new project: LinuxKit . Being able to run containers on many different platforms is one advantage of using the Docker tooling with containers. In addition to running linux containers on Windows using a linux subsystem, native Windows containers are now possible due the creation of container primitives on the Windows OS. Native Windows containers can be run on Windows 10 or Windows Server 2016 or newer. Note : if you run this exercise in a containerized terminal and execute the ps -ef command in the terminal, e.g. in https://labs.cognitiveclass.ai , you will still see a limited set of processes after exiting the exec command. You can try to run the ps -ef command in a terminal on your local machine to see all processes. Clean up the container running the top processes by typing: <ctrl>-c , list all containers and remove the containers by their id. docker ps -a docker rm <CONTAINER ID>","title":"Step 1: Run your first container"},{"location":"generatedContent/docker101/lab-1/#step-2-run-multiple-containers","text":"Explore the Docker Hub The Docker Hub is the public central registry for Docker images, which contains community and official images. When searching for images you will find filters for \"Docker Certified\", \"Verified Publisher\" and \"Official Images\" images. Select the \"Docker Certified\" filter, to find images that are deemed enterprise-ready and are tested with Docker Enterprise Edition product. It is important to avoid using unverified content from the Docker Store when developing your own images that are intended to be deployed into the production environment. These unverified images may contain security vulnerabilities or possibly even malicious software. In Step 2 of this lab, we will start a couple of containers using some verified images from the Docker Hub: nginx web server, and mongo database. Run an Nginx server Let's run a container using the official Nginx image from the Docker Hub. $ docker container run --detach --publish 8080 :80 --name nginx nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 36a46ebd5019: Pull complete 57168433389f: Pull complete 332ec8285c50: Pull complete Digest: sha256:c15f1fb8fd55c60c72f940a76da76a5fccce2fefa0dd9b17967b9e40b0355316 Status: Downloaded newer image for nginx:latest 5e1bf0e6b926bd73a66f98b3cbe23d04189c16a43d55dd46b8486359f6fdf048 We are using a couple of new flags here. The --detach flag will run this container in the background. The publish flag publishes port 80 in the container (the default port for nginx), via port 8080 on our host. Remember that the NET namespace gives processes of the container their own network stack. The --publish flag is a feature that allows us to expose networking through the container onto the host. How do you know port 80 is the default port for nginx? Because it is listed in the documentation on the Docker Hub. In general, the documentation for the verified images is very good, and you will want to refer to them when running containers using those images. We are also specifying the --name flag, which names the container. Every container has a name, if you don't specify one, Docker will randomly assign one for you. Specifying your own name makes it easier to run subsequent commands on your container since you can reference the name instead of the id of the container. For example: docker container inspect nginx instead of docker container inspect 5e1 . Since this is the first time you are running the nginx container, it will pull down the nginx image from the Docker Store. Subsequent containers created from the Nginx image will use the existing image located on your host. Nginx is a lightweight web server. You can access it on port 8080 on your localhost. Access the nginx server on localhost:8080 . curl localhost:8080 will return the HTML home page of Nginx, <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> If you are using play-with-docker, look for the 8080 link near the top of the page, or if you run a Docker client with access to a local browser, Run a mongo DB server Now, run a mongoDB server. We will use the official mongoDB image from the Docker Hub. Instead of using the latest tag (which is the default if no tag is specified), we will use a specific version of the mongo image: 4.4. $ docker container run --detach --publish 8081 :27017 --name mongo mongo:4.4 Unable to find image mongo:4.4 locally 4 .4: Pulling from library/mongo d13d02fa248d: Already exists bc8e2652ce92: Pull complete 3cc856886986: Pull complete c319e9ec4517: Pull complete b4cbf8808f94: Pull complete cb98a53e6676: Pull complete f0485050cd8a: Pull complete ac36cdc414b3: Pull complete 61814e3c487b: Pull complete 523a9f1da6b9: Pull complete 3b4beaef77a2: Pull complete Digest: sha256:d13c897516e497e898c229e2467f4953314b63e48d4990d3215d876ef9d1fc7c Status: Downloaded newer image for mongo:4.4 d8f614a4969fb1229f538e171850512f10f490cb1a96fca27e4aa89ac082eba5 Again, since this is the first time we are running a mongo container, we will pull down the mongo image from the Docker Store. We are using the --publish flag to expose the 27017 mongo port on our host. We have to use a port other than 8080 for the host mapping, since that port is already exposed on our host. Again refer to the official docs on the Docker Hub to get more details about using the mongo image. Access localhost:8081 to see some output from mongo. curl localhost:8081 which will return a warning from MongoDB, It looks like you are trying to access MongoDB over HTTP on the native driver port. If you are using play-with-docker, look for the 8080 link near the top of the page. Check your running containers with docker container ls $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6777df89fea nginx \"nginx -g 'daemon ...\" Less than a second ago Up 2 seconds 0 .0.0.0:8080->80/tcp nginx ead80a0db505 mongo \"docker-entrypoint...\" 17 seconds ago Up 19 seconds 0 .0.0.0:8081->27017/tcp mongo af549dccd5cf ubuntu \"top\" 5 minutes ago Up 5 minutes priceless_kepler You should see that you have an Nginx web server container, and a MongoDB container running on your host. Note that we have not configured these containers to talk to each other. You can see the \"nginx\" and \"mongo\" names that we gave to our containers, and the random name (in my case \"priceless_kepler\") that was generated for the ubuntu container. You can also see that the port mappings that we specified with the --publish flag. For more details information on these running containers you can use the docker container inspect [container id command. One thing you might notice is that the mongo container is running the docker-entrypoint command. This is the name of the executable that is run when the container is started. The mongo image requires some prior configuration before kicking off the DB process. You can see exactly what the script does by looking at it on github . Typically, you can find the link to the github source from the image description page on the Docker Store website. Containers are self-contained and isolated, which means we can avoid potential conflicts between containers with different system or runtime dependencies. For example: deploying an app that uses Java 7 and another app that uses Java 8 on the same host. Or running multiple nginx containers that all have port 80 as their default listening ports (if exposing on the host using the --publish flag, the ports selected for the host will need to be unique). Isolation benefits are possible because of Linux Namespaces. Note : You didn't have to install anything on your host (other than Docker) to run these processes! Each container includes the dependencies that it needs within the container, so you don't need to install anything on your host directly. Running multiple containers on the same host gives us the ability to fully utilize the resources (cpu, memory, etc) available on single host. This can result in huge cost savings for an enterprise. While running images directly from the Docker Hub can be useful at times, it is more useful to create custom images, and refer to official images as the starting point for these images. We will dive into building our own custom images in Lab 2.","title":"Step 2: Run Multiple Containers"},{"location":"generatedContent/docker101/lab-1/#step-3-clean-up","text":"Completing this lab results in a bunch of running containers on your host. Let's clean these up. First get a list of the containers running using docker container ls . $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6777df89fea nginx \"nginx -g 'daemon ...\" 3 minutes ago Up 3 minutes 0 .0.0.0:8080->80/tcp nginx ead80a0db505 mongo \"docker-entrypoint...\" 3 minutes ago Up 3 minutes 0 .0.0.0:8081->27017/tcp mongo af549dccd5cf ubuntu \"top\" 8 minutes ago Up 8 minutes priceless_kepler Next, run docker container stop [container id] for each container in the list. You can also use the names of the containers that you specified before. $ docker container stop d67 ead af5 d67 ead af5 Note : You only have to reference enough digits of the ID to be unique. Three digits is almost always enough. Remove the stopped containers docker system prune is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images. $ docker system prune WARNING! This will remove: - all stopped containers - all volumes not used by at least one container - all networks not used by at least one container - all dangling images Are you sure you want to continue ? [ y/N ] y Deleted Containers: 7872fd96ea4695795c41150a06067d605f69702dbcb9ce49492c9029f0e1b44b 60abd5ee65b1e2732ddc02b971a86e22de1c1c446dab165462a08b037ef7835c 31617fdd8e5f584c51ce182757e24a1c9620257027665c20be75aa3ab6591740 Total reclaimed space: 12B","title":"Step 3: Clean Up"},{"location":"generatedContent/docker101/lab-1/#summary","text":"In this lab, you created your first Ubuntu, Nginx and MongoDB containers. Key Takeaways Containers are composed of linux namespaces and control groups that provide isolation from other containers and the host. Because of the isolation properties of containers, you can schedule many containers on a single host without worrying about conflicting dependencies. This makes it easier to run multiple containers on a single host: fully utilizing resources allocated to that host, and ultimately saving some money on server costs. Avoid using unverified content from the Docker Store when developing your own images because these images may contain security vulnerabilities or possibly even malicious software. Containers include everything they need to run the processes within them, so there is no need to install additional dependencies directly on your host.","title":"Summary"},{"location":"generatedContent/docker101/lab-2/","text":"Lab 2 - Adding Value with Custom Docker Images \u00b6 Overview \u00b6 In this lab, we build on our knowledge from lab 1 where we used Docker commands to run containers. We will create a custom Docker Image built from a Dockerfile. Once we build the image, we will push it to a central registry where it can be pulled to be deployed on other environments. Also, we will briefly describe image layers, and how Docker incorporates \"copy-on-write\" and the union file system to efficiently store images and run containers. We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation . Prerequisites \u00b6 Completed Lab 0: You must have access to a docker client, either on localhost, use a terminal from Theia - Cloud IDE at https://labs.cognitiveclass.ai/tools/theiadocker or be using Play with Docker for example. Step 1: Create a python app (without using Docker) \u00b6 Run the following command to create a file named app.py with a simple python program. (copy-paste the entire code block) echo 'from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"hello world!\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\")' > app.py This is a simple python app that uses flask to expose a http web server on port 5000 (5000 is the default port for flask). Don't worry if you are not too familiar with python or flask, these concepts can be applied to an application written in any language. Optional: If you have python and pip installed, you can run this app locally. If not, move on to the next step. $ python3 --version Python 3.6.9 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) $ pip3 install flask Collecting flask Downloading https://files.pythonhosted.org/packages/f2/28/2a03252dfb9ebf377f40fba6a7841b47083260bf8bd8e737b0c6952df83f/Flask-1.1.2-py2.py3-none-any.whl (94kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 6.2MB/s Collecting Werkzeug>=0.15 (from flask) Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 307kB 3.4MB/s Collecting itsdangerous>=0.24 (from flask) Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl Collecting click>=5.1 (from flask) Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 10.2MB/s Collecting Jinja2>=2.10.1 (from flask) Downloading https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl (125kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.5MB/s Collecting MarkupSafe>=0.23 (from Jinja2>=2.10.1->flask) Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl Installing collected packages: Werkzeug, itsdangerous, click, MarkupSafe, Jinja2, flask Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0 $ python3 app.py * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) Step 2: Create and build the Docker Image \u00b6 Now, what if you don't have python installed locally? Don't worry! Because you don't need it. One of the advantages of using containers is that you can build python inside your containers, without having python installed on your host machine. Create a Dockerfile but running the following command. (copy-paste the entire code block) echo 'FROM python:3.8-alpine RUN pip install flask CMD [\"python\",\"app.py\"] COPY app.py /app.py' > Dockerfile A Dockerfile lists the instructions needed to build a docker image. Let's go through the above file line by line. FROM python:3.8-alpine This is the starting point for your Dockerfile. Every Dockerfile must start with a FROM line that is the starting image to build your layers on top of. In this case, we are selecting the python:3.8-alpine base layer (see Dockerfile for python3.8/alpine3.12 ) since it already has the version of python and pip that we need to run our application. The alpine version means that it uses the Alpine Linux distribution, which is significantly smaller than many alternative flavors of Linux, around 8 MB in size, while a minimal installation to disk might be around 130 MB. A smaller image means it will download (deploy) much faster, and it also has advantages for security because it has a smaller attack surface. Alpine Linux is a Linux distribution based on musl and BusyBox. Here we are using the \"3.8-alpine\" tag for the python image. Take a look at the available tags for the official python image on the Docker Hub . It is best practice to use a specific tag when inheriting a parent image so that changes to the parent dependency are controlled. If no tag is specified, the \"latest\" tag takes into effect, which is acts as a dynamic pointer that points to the latest version of an image. For security reasons, it is very important to understand the layers that you build your docker image on top of. For that reason, it is highly recommended to only use \"official\" images found in the docker hub , or non-community images found in the docker-store. These images are vetted to meet certain security requirements, and also have very good documentation for users to follow. You can find more information about this python base image , as well as all other images that you can use, on the docker hub . For a more complex application you may find the need to use a FROM image that is higher up the chain. For example, the parent Dockerfile for our python app starts with FROM alpine , then specifies a series of CMD and RUN commands for the image. If you needed more fine-grained control, you could start with FROM alpine (or a different distribution) and run those steps yourself. To start off though, I recommend using an official image that closely matches your needs. RUN pip install flask The RUN command executes commands needed to set up your image for your application, such as installing packages, editing files, or changing file permissions. In this case we are installing flask. The RUN commands are executed at build time, and are added to the layers of your image. CMD [\"python\",\"app.py\"] CMD is the command that is executed when you start a container. Here we are using CMD to run our python app. There can be only one CMD per Dockerfile. If you specify more thane one CMD , then the last CMD will take effect. The parent python:3.8-alpine also specifies a CMD ( CMD python3 ). You can find the Dockerfile for the official python:alpine image here . You can use the official python image directly to run python scripts without installing python on your host. But today, we are creating a custom image to include our source, so that we can build an image with our application and ship it around to other environments. COPY app.py /app.py This copies the app.py in the local directory (where you will run docker image build ) into a new layer of the image. This instruction is the last line in the Dockerfile. Layers that change frequently, such as copying source code into the image, should be placed near the bottom of the file to take full advantage of the Docker layer cache. This allows us to avoid rebuilding layers that could otherwise be cached. For instance, if there was a change in the FROM instruction, it would invalidate the cache for all subsequent layers of this image. We will demonstrate a this little later in this lab. It seems counter-intuitive to put this after the CMD [\"python\",\"app.py\"] line. Remember, the CMD line is executed only when the container is started, so we won't get a file not found error here. And there you have it: a very simple Dockerfile. A full list of commands you can put into a Dockerfile can be found here . Now that we defined our Dockerfile, let's use it to build our custom docker image. Build the docker image. Pass in -t to name your image python-hello-world . $ docker image build -t python-hello-world . Sending build context to Docker daemon 3 .072kB Step 1 /4 : FROM python:3.8-alpine 3 .8-alpine: Pulling from library/python df20fa9351a1: Pull complete 36b3adc4ff6f: Pull complete 3e7ef1bb9eba: Pull complete 78538f72d6a9: Pull complete 07bc731e0055: Pull complete Digest: sha256:cbc08bfc4b1b732076742f52852ede090e960ab7470d0a60ee4f964cfa7c710a Status: Downloaded newer image for python:3.8-alpine ---> 0f03316d4a27 Step 2 /4 : RUN pip install flask ---> Running in 1454bdd1ea98 Collecting flask Downloading Flask-1.1.2-py2.py3-none-any.whl ( 94 kB ) Collecting itsdangerous> = 0 .24 Downloading itsdangerous-1.1.0-py2.py3-none-any.whl ( 16 kB ) Collecting Werkzeug> = 0 .15 Downloading Werkzeug-1.0.1-py2.py3-none-any.whl ( 298 kB ) Collecting click> = 5 .1 Downloading click-7.1.2-py2.py3-none-any.whl ( 82 kB ) Collecting Jinja2> = 2 .10.1 Downloading Jinja2-2.11.2-py2.py3-none-any.whl ( 125 kB ) Collecting MarkupSafe> = 0 .23 Downloading MarkupSafe-1.1.1.tar.gz ( 19 kB ) Building wheels for collected packages: MarkupSafe Building wheel for MarkupSafe ( setup.py ) : started Building wheel for MarkupSafe ( setup.py ) : finished with status 'done' Created wheel for MarkupSafe: filename = MarkupSafe-1.1.1-py3-none-any.whl size = 12627 sha256 = 155e3314602dfac3c8ea245edc217c235afb4c818932574d6d61529ef0c14ea4 Stored in directory: /root/.cache/pip/wheels/0c/61/d6/4db4f4c28254856e82305fdb1f752ed7f8482e54c384d8cb0e Successfully built MarkupSafe Installing collected packages: itsdangerous, Werkzeug, click, MarkupSafe, Jinja2, flask Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0 Removing intermediate container 1454bdd1ea98 ---> 97d747fc7771 Step 3 /4 : CMD [ \"python\" , \"app.py\" ] ---> Running in e2bf74801c81 Removing intermediate container e2bf74801c81 ---> d5adbccf5116 Step 4 /4 : COPY app.py /app.py ---> 3c24958f29d3 Successfully built 3c24958f29d3 Successfully tagged python-hello-world:latest Verify that your image shows up in your image list via docker image ls . $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE python-hello-world latest 3c24958f29d3 52 seconds ago 53 .4MB python 3 .8-alpine 0f03316d4a27 2 weeks ago 42 .7MB Note that your base image python:3.8-alpine is also in your list. You can run a history command to show the history of an image and its layers, docker history python-hello-world docker history python:3.8-alpine Step 3: Run the Docker image \u00b6 Now that you have built the image, you can run it to see that it works. Run the Docker image $ docker run -p 5001 :5000 -d python-hello-world 0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26 The -p flag maps a port running inside the container to your host. In this case, we are mapping the python app running on port 5000 inside the container, to port 5001 on your host. Note that if port 5001 is already in use by another application on your host, you may have to replace 5001 with another value, such as 5002. Navigate to localhost:5001 in a browser to see the results. In a terminal run, curl localhost:5001 which returns hello world! . If you are using katacoda, click on the link in the left-hand pane that says: View port at https://....environments.katacoda.com then type in 5001 and click Display Port . In play-with-docker, click the link 5001 that should appear near the top of your session. You should see \"hello world!\" on your browser. Check the log output of the container. If you want to see logs from your application you can use the docker container logs command. By default, docker container logs prints out what is sent to standard out by your application. Use docker container ls to find the id for your running container. sh $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7b04d5320cb4 python-hello-world \"python app.py\" About a minute ago Up About a minute 0.0.0.0:5001->5000/tcp elastic_ganguly $ docker container logs [container id] * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) 172.17.0.1 - - [23/Sep/2020 22:00:33] \"GET / HTTP/1.1\" 200 - The Dockerfile is how you create reproducible builds for your application. A common workflow is to have your CI/CD automation run docker image build as part of its build process. Once images are built, they will be sent to a central registry, where it can be accessed by all environments (such as a test environment) that need to run instances of that application. In the next step, we will push our custom image to the public docker registry: the docker hub, where it can be consumed by other developers and operators. Step 4: Push to a central registry \u00b6 Navigate to Docker Hub and create an account if you haven't already. Alternatively, you can also use https://quay.io for instance. For this lab we will be using the docker hub as our central registry. Docker hub is a free service to store publicly available images, or you can pay to store private images. Go to the Docker Hub website and create a free account. Most organizations that use docker heavily will set up their own registry internally. To simplify things, we will be using the Docker Hub, but the following concepts apply to any registry. Login You can log into the image registry account by typing docker login on your terminal, or if using podman, type podman login . $ export DOCKERHUB_USERNAME = <dockerhub-username> $ docker login docker.io -u $DOCKERHUB_USERNAME password: WARNING! Your password will be stored unencrypted in /home/theia/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Tag your image with your username The Docker Hub naming convention is to tag your image with [dockerhub username]/[image name]. To do this, we are going to tag our previously created image python-hello-world to fit that format. docker tag python-hello-world $DOCKERHUB_USERNAME /python-hello-world:1.0.0 Push your image to the registry Once we have a properly tagged image, we can use the docker push command to push our image to the Docker Hub registry. $ docker push $DOCKERHUB_USERNAME /python-hello-world:1.0.0 The push refers to a repository [ docker.io/jzaccone/python-hello-world ] 2bce026769ac: Pushed 64d445ecbe93: Pushed 18b27eac38a1: Mounted from library/python 3f6f25cd8b1e: Mounted from library/python b7af9d602a0f: Mounted from library/python ed06208397d5: Mounted from library/python 5accac14015f: Mounted from library/python latest: digest: sha256:508238f264616bf7bf962019d1a3826f8487ed6a48b80bf41fd3996c7175fd0f size: 1786 Check out your image on docker hub in your browser Navigate to Docker Hub and go to your profile to see your newly uploaded image at https://hub.docker.com/repository/docker/<dockerhub-username>/python-hello-world . Now that your image is on Docker Hub, other developers and operations can use the docker pull command to deploy your image to other environments. Note: Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to. We also don't have to go through additional steps to provision these environments. Just one step: install docker, and you are good to go. Run from Docker Hub, docker run -p 5001 :5000 -d $DOCKERHUB_USERNAME /python-hello-world:1.0.0 In a terminal run, curl localhost:5001 which should return hello world! again. Step 5: Deploying a Change \u00b6 The \"hello world!\" application is overrated, let's update the app so that it says \"Hello Beautiful World!\" instead. Update app.py Replace the string \"Hello World\" with \"Hello Beautiful World!\" in app.py . You can update the file with the following command. (copy-paste the entire code block) echo 'from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"hello beautiful world!\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\")' > app.py Rebuild and push your image Now that your app is updated, you need repeat the steps above to rebuild your app and push it to the Docker Hub registry. First rebuild, this time use your Docker Hub username in the build command: $ docker image build -t $DOCKERHUB_USERNAME /python-hello-world . Sending build context to Docker daemon 3 .072kB Step 1 /4 : FROM python:3.6.1-alpine ---> c86415c03c37 Step 2 /4 : RUN pip install flask ---> Using cache ---> ce41f2517c16 Step 3 /4 : CMD python app.py ---> Using cache ---> 0ab91286958b Step 4 /4 : COPY app.py /app.py ---> 3e08b2eeace1 Removing intermediate container 23a955e881fc Successfully built 3e08b2eeace1 Successfully tagged <dockerhub-username>/python-hello-world:latest Notice the \"Using cache\" for steps 1-3. These layers of the Docker Image have already been built and docker image build will use these layers from the cache instead of rebuilding them. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/<dockerhub-username>/python-hello-world ] 94525867566e: Pushed 64d445ecbe93: Layer already exists 18b27eac38a1: Layer already exists 3f6f25cd8b1e: Layer already exists b7af9d602a0f: Layer already exists ed06208397d5: Layer already exists 5accac14015f: Layer already exists latest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786 There is a caching mechanism in place for pushing layers too. Docker Hub already has all but one of the layers from an earlier push, so it only pushes the one layer that has changed. When you change a layer, every layer built on top of that will have to be rebuilt. Each line in a Dockerfile builds a new layer that is built on the layer created from the lines before it. This is why the order of the lines in our Dockerfile is important. We optimized our Dockerfile so that the layer that is most likely to change ( COPY app.py /app.py ) is the last line of the Dockerfile. Generally for an application, your code changes at the most frequent rate. This optimization is particularly important for CI/CD processes, where you want your automation to run as fast as possible. Step 6: Understanding Image Layers \u00b6 One of the major design properties of Docker is its use of the union file system. Consider the Dockerfile that we created before: FROM python:3.8-alpine RUN pip install flask CMD [ \"python\" , \"app.py\" ] COPY app.py /app.py Each of these lines is a layer. Each layer contains only the delta, diff or changes from the layers before it. To put these layers together into a single running container, Docker makes use of the union file system to overlay layers transparently into a single view. Each layer of the image is read-only , except for the very top layer which is created for the running container. The read/write container layer implements \"copy-on-write\" which means that files that are stored in lower image layers are pulled up to the read/write container layer only when edits are being made to those files. Those changes are then stored in the running container layer. The \"copy-on-write\" function is very fast, and in almost all cases, does not have a noticeable effect on performance. You can inspect which files have been pulled up to the container level with the docker diff command. More information about how to use docker diff can be found here . Since image layers are read-only , they can be shared by images and by running containers. For instance, creating a new python app with its own Dockerfile with similar base layers, would share all the layers that it had in common with the first python app. FROM python:3.8-alpine RUN pip install flask CMD [ \"python\" , \"app2.py\" ] COPY app2.py /app2.py You can also experience the sharing of layers when you start multiple containers from the same image. Since the containers use the same read-only layers, you can imagine that starting up containers is very fast and has a very low footprint on the host. You may notice that there are duplicate lines in this Dockerfile and the Dockerfile you created earlier in this lab. Although this is a very trivial example, you can pull common lines of both Dockerfiles into a \"base\" Dockerfile, that you can then point to with each of your child Dockerfiles using the FROM command. Image layering enables the docker caching mechanism for builds and pushes. For example, the output for your last docker push shows that some of the layers of your image already exists on the Docker Hub. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/<dockerhub-username>/python-hello-world ] 94525867566e: Pushed 64d445ecbe93: Layer already exists 18b27eac38a1: Layer already exists 3f6f25cd8b1e: Layer already exists b7af9d602a0f: Layer already exists ed06208397d5: Layer already exists 5accac14015f: Layer already exists latest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786 To look more closely at layers, you can use the docker image history command of the python image we created. $ docker image history python-hello-world IMAGE CREATED CREATED BY SIZE COMMENT 3c24958f29d3 17 minutes ago /bin/sh -c #(nop) COPY file:5fef1b9a6220c0e3\u2026 159B d5adbccf5116 17 minutes ago /bin/sh -c #(nop) CMD [\"python\" \"app.py\"] 0B 97d747fc7771 17 minutes ago /bin/sh -c pip install flask 10.7MB 0f03316d4a27 2 weeks ago /bin/sh -c #(nop) CMD [\"python3\"] 0B <missing> 2 weeks ago /bin/sh -c set -ex; wget -O get-pip.py \"$P\u2026 7.24MB <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_GET_PIP_SHA256\u2026 0B <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_GET_PIP_URL=ht\u2026 0B <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_PIP_VERSION=20\u2026 0B <missing> 7 weeks ago /bin/sh -c cd /usr/local/bin && ln -s idle3\u2026 32B <missing> 7 weeks ago /bin/sh -c set -ex && apk add --no-cache --\u2026 29.3MB <missing> 2 months ago /bin/sh -c #(nop) ENV PYTHON_VERSION=3.8.5 0B <missing> 3 months ago /bin/sh -c #(nop) ENV GPG_KEY=E3FF2839C048B\u2026 0B <missing> 3 months ago /bin/sh -c apk add --no-cache ca-certificates 512kB <missing> 3 months ago /bin/sh -c #(nop) ENV LANG=C.UTF-8 0B <missing> 3 months ago /bin/sh -c #(nop) ENV PATH=/usr/local/bin:/\u2026 0B <missing> 3 months ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 3 months ago /bin/sh -c #(nop) ADD file:c92c248239f8c7b9b\u2026 5.57MB Each line represents a layer of the image. You'll notice that the top lines match to your Dockerfile that you created, and the lines below are pulled from the parent python image. Don't worry about the \"\\<missing>\" tags. These are still normal layers; they have just not been given an ID by the docker system. Step 7: Clean up \u00b6 Completing this lab results in a bunch of running containers on your host. Let's clean these up. Run docker container stop [container id] for each container that is running First get a list of the containers running using docker container ls . $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0b2ba61df37f python-hello-world \"python app.py\" 7 minutes ago Up 7 minutes 0 .0.0.0:5001->5000/tcp practical_kirch Then run docker container stop [container id] for each container in the list. $ docker container stop 0b2 0b2 Remove the stopped containers docker system prune is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images. $ docker system prune WARNING! This will remove: - all stopped containers - all volumes not used by at least one container - all networks not used by at least one container - all dangling images Are you sure you want to continue ? [ y/N ] y Deleted Containers: 0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26 Total reclaimed space: 300 .3kB Summary \u00b6 In this lab, you started adding value by creating your own custom docker containers. Key Takeaways: The Dockerfile is how you create reproducible builds for your application and how you integrate your application with Docker into the CI/CD pipeline Docker images can be made available to all of your environments through a central registry. The Docker Hub is one example of a registry, but you can deploy your own registry on servers you control. Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to. Docker makes use of the union file system and \"copy on write\" to reuse layers of images. This lowers the footprint of storing images and significantly increases the performance of starting containers. Image layers are cached by the Docker build and push system. No need to rebuild or repush image layers that are already present on the desired system. Each line in a Dockerfile creates a new layer, and because of the layer cache, the lines that change more frequently (e.g. adding source code to an image) should be listed near the bottom of the file.","title":"Lab 2. Create Custom Images"},{"location":"generatedContent/docker101/lab-2/#lab-2-adding-value-with-custom-docker-images","text":"","title":"Lab 2 - Adding Value with Custom Docker Images"},{"location":"generatedContent/docker101/lab-2/#overview","text":"In this lab, we build on our knowledge from lab 1 where we used Docker commands to run containers. We will create a custom Docker Image built from a Dockerfile. Once we build the image, we will push it to a central registry where it can be pulled to be deployed on other environments. Also, we will briefly describe image layers, and how Docker incorporates \"copy-on-write\" and the union file system to efficiently store images and run containers. We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation .","title":"Overview"},{"location":"generatedContent/docker101/lab-2/#prerequisites","text":"Completed Lab 0: You must have access to a docker client, either on localhost, use a terminal from Theia - Cloud IDE at https://labs.cognitiveclass.ai/tools/theiadocker or be using Play with Docker for example.","title":"Prerequisites"},{"location":"generatedContent/docker101/lab-2/#step-1-create-a-python-app-without-using-docker","text":"Run the following command to create a file named app.py with a simple python program. (copy-paste the entire code block) echo 'from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"hello world!\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\")' > app.py This is a simple python app that uses flask to expose a http web server on port 5000 (5000 is the default port for flask). Don't worry if you are not too familiar with python or flask, these concepts can be applied to an application written in any language. Optional: If you have python and pip installed, you can run this app locally. If not, move on to the next step. $ python3 --version Python 3.6.9 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) $ pip3 install flask Collecting flask Downloading https://files.pythonhosted.org/packages/f2/28/2a03252dfb9ebf377f40fba6a7841b47083260bf8bd8e737b0c6952df83f/Flask-1.1.2-py2.py3-none-any.whl (94kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 6.2MB/s Collecting Werkzeug>=0.15 (from flask) Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 307kB 3.4MB/s Collecting itsdangerous>=0.24 (from flask) Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl Collecting click>=5.1 (from flask) Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 10.2MB/s Collecting Jinja2>=2.10.1 (from flask) Downloading https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl (125kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.5MB/s Collecting MarkupSafe>=0.23 (from Jinja2>=2.10.1->flask) Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl Installing collected packages: Werkzeug, itsdangerous, click, MarkupSafe, Jinja2, flask Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0 $ python3 app.py * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)","title":"Step 1: Create a python app (without using Docker)"},{"location":"generatedContent/docker101/lab-2/#step-2-create-and-build-the-docker-image","text":"Now, what if you don't have python installed locally? Don't worry! Because you don't need it. One of the advantages of using containers is that you can build python inside your containers, without having python installed on your host machine. Create a Dockerfile but running the following command. (copy-paste the entire code block) echo 'FROM python:3.8-alpine RUN pip install flask CMD [\"python\",\"app.py\"] COPY app.py /app.py' > Dockerfile A Dockerfile lists the instructions needed to build a docker image. Let's go through the above file line by line. FROM python:3.8-alpine This is the starting point for your Dockerfile. Every Dockerfile must start with a FROM line that is the starting image to build your layers on top of. In this case, we are selecting the python:3.8-alpine base layer (see Dockerfile for python3.8/alpine3.12 ) since it already has the version of python and pip that we need to run our application. The alpine version means that it uses the Alpine Linux distribution, which is significantly smaller than many alternative flavors of Linux, around 8 MB in size, while a minimal installation to disk might be around 130 MB. A smaller image means it will download (deploy) much faster, and it also has advantages for security because it has a smaller attack surface. Alpine Linux is a Linux distribution based on musl and BusyBox. Here we are using the \"3.8-alpine\" tag for the python image. Take a look at the available tags for the official python image on the Docker Hub . It is best practice to use a specific tag when inheriting a parent image so that changes to the parent dependency are controlled. If no tag is specified, the \"latest\" tag takes into effect, which is acts as a dynamic pointer that points to the latest version of an image. For security reasons, it is very important to understand the layers that you build your docker image on top of. For that reason, it is highly recommended to only use \"official\" images found in the docker hub , or non-community images found in the docker-store. These images are vetted to meet certain security requirements, and also have very good documentation for users to follow. You can find more information about this python base image , as well as all other images that you can use, on the docker hub . For a more complex application you may find the need to use a FROM image that is higher up the chain. For example, the parent Dockerfile for our python app starts with FROM alpine , then specifies a series of CMD and RUN commands for the image. If you needed more fine-grained control, you could start with FROM alpine (or a different distribution) and run those steps yourself. To start off though, I recommend using an official image that closely matches your needs. RUN pip install flask The RUN command executes commands needed to set up your image for your application, such as installing packages, editing files, or changing file permissions. In this case we are installing flask. The RUN commands are executed at build time, and are added to the layers of your image. CMD [\"python\",\"app.py\"] CMD is the command that is executed when you start a container. Here we are using CMD to run our python app. There can be only one CMD per Dockerfile. If you specify more thane one CMD , then the last CMD will take effect. The parent python:3.8-alpine also specifies a CMD ( CMD python3 ). You can find the Dockerfile for the official python:alpine image here . You can use the official python image directly to run python scripts without installing python on your host. But today, we are creating a custom image to include our source, so that we can build an image with our application and ship it around to other environments. COPY app.py /app.py This copies the app.py in the local directory (where you will run docker image build ) into a new layer of the image. This instruction is the last line in the Dockerfile. Layers that change frequently, such as copying source code into the image, should be placed near the bottom of the file to take full advantage of the Docker layer cache. This allows us to avoid rebuilding layers that could otherwise be cached. For instance, if there was a change in the FROM instruction, it would invalidate the cache for all subsequent layers of this image. We will demonstrate a this little later in this lab. It seems counter-intuitive to put this after the CMD [\"python\",\"app.py\"] line. Remember, the CMD line is executed only when the container is started, so we won't get a file not found error here. And there you have it: a very simple Dockerfile. A full list of commands you can put into a Dockerfile can be found here . Now that we defined our Dockerfile, let's use it to build our custom docker image. Build the docker image. Pass in -t to name your image python-hello-world . $ docker image build -t python-hello-world . Sending build context to Docker daemon 3 .072kB Step 1 /4 : FROM python:3.8-alpine 3 .8-alpine: Pulling from library/python df20fa9351a1: Pull complete 36b3adc4ff6f: Pull complete 3e7ef1bb9eba: Pull complete 78538f72d6a9: Pull complete 07bc731e0055: Pull complete Digest: sha256:cbc08bfc4b1b732076742f52852ede090e960ab7470d0a60ee4f964cfa7c710a Status: Downloaded newer image for python:3.8-alpine ---> 0f03316d4a27 Step 2 /4 : RUN pip install flask ---> Running in 1454bdd1ea98 Collecting flask Downloading Flask-1.1.2-py2.py3-none-any.whl ( 94 kB ) Collecting itsdangerous> = 0 .24 Downloading itsdangerous-1.1.0-py2.py3-none-any.whl ( 16 kB ) Collecting Werkzeug> = 0 .15 Downloading Werkzeug-1.0.1-py2.py3-none-any.whl ( 298 kB ) Collecting click> = 5 .1 Downloading click-7.1.2-py2.py3-none-any.whl ( 82 kB ) Collecting Jinja2> = 2 .10.1 Downloading Jinja2-2.11.2-py2.py3-none-any.whl ( 125 kB ) Collecting MarkupSafe> = 0 .23 Downloading MarkupSafe-1.1.1.tar.gz ( 19 kB ) Building wheels for collected packages: MarkupSafe Building wheel for MarkupSafe ( setup.py ) : started Building wheel for MarkupSafe ( setup.py ) : finished with status 'done' Created wheel for MarkupSafe: filename = MarkupSafe-1.1.1-py3-none-any.whl size = 12627 sha256 = 155e3314602dfac3c8ea245edc217c235afb4c818932574d6d61529ef0c14ea4 Stored in directory: /root/.cache/pip/wheels/0c/61/d6/4db4f4c28254856e82305fdb1f752ed7f8482e54c384d8cb0e Successfully built MarkupSafe Installing collected packages: itsdangerous, Werkzeug, click, MarkupSafe, Jinja2, flask Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0 Removing intermediate container 1454bdd1ea98 ---> 97d747fc7771 Step 3 /4 : CMD [ \"python\" , \"app.py\" ] ---> Running in e2bf74801c81 Removing intermediate container e2bf74801c81 ---> d5adbccf5116 Step 4 /4 : COPY app.py /app.py ---> 3c24958f29d3 Successfully built 3c24958f29d3 Successfully tagged python-hello-world:latest Verify that your image shows up in your image list via docker image ls . $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE python-hello-world latest 3c24958f29d3 52 seconds ago 53 .4MB python 3 .8-alpine 0f03316d4a27 2 weeks ago 42 .7MB Note that your base image python:3.8-alpine is also in your list. You can run a history command to show the history of an image and its layers, docker history python-hello-world docker history python:3.8-alpine","title":"Step 2: Create and build the Docker Image"},{"location":"generatedContent/docker101/lab-2/#step-3-run-the-docker-image","text":"Now that you have built the image, you can run it to see that it works. Run the Docker image $ docker run -p 5001 :5000 -d python-hello-world 0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26 The -p flag maps a port running inside the container to your host. In this case, we are mapping the python app running on port 5000 inside the container, to port 5001 on your host. Note that if port 5001 is already in use by another application on your host, you may have to replace 5001 with another value, such as 5002. Navigate to localhost:5001 in a browser to see the results. In a terminal run, curl localhost:5001 which returns hello world! . If you are using katacoda, click on the link in the left-hand pane that says: View port at https://....environments.katacoda.com then type in 5001 and click Display Port . In play-with-docker, click the link 5001 that should appear near the top of your session. You should see \"hello world!\" on your browser. Check the log output of the container. If you want to see logs from your application you can use the docker container logs command. By default, docker container logs prints out what is sent to standard out by your application. Use docker container ls to find the id for your running container. sh $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7b04d5320cb4 python-hello-world \"python app.py\" About a minute ago Up About a minute 0.0.0.0:5001->5000/tcp elastic_ganguly $ docker container logs [container id] * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) 172.17.0.1 - - [23/Sep/2020 22:00:33] \"GET / HTTP/1.1\" 200 - The Dockerfile is how you create reproducible builds for your application. A common workflow is to have your CI/CD automation run docker image build as part of its build process. Once images are built, they will be sent to a central registry, where it can be accessed by all environments (such as a test environment) that need to run instances of that application. In the next step, we will push our custom image to the public docker registry: the docker hub, where it can be consumed by other developers and operators.","title":"Step 3: Run the Docker image"},{"location":"generatedContent/docker101/lab-2/#step-4-push-to-a-central-registry","text":"Navigate to Docker Hub and create an account if you haven't already. Alternatively, you can also use https://quay.io for instance. For this lab we will be using the docker hub as our central registry. Docker hub is a free service to store publicly available images, or you can pay to store private images. Go to the Docker Hub website and create a free account. Most organizations that use docker heavily will set up their own registry internally. To simplify things, we will be using the Docker Hub, but the following concepts apply to any registry. Login You can log into the image registry account by typing docker login on your terminal, or if using podman, type podman login . $ export DOCKERHUB_USERNAME = <dockerhub-username> $ docker login docker.io -u $DOCKERHUB_USERNAME password: WARNING! Your password will be stored unencrypted in /home/theia/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Tag your image with your username The Docker Hub naming convention is to tag your image with [dockerhub username]/[image name]. To do this, we are going to tag our previously created image python-hello-world to fit that format. docker tag python-hello-world $DOCKERHUB_USERNAME /python-hello-world:1.0.0 Push your image to the registry Once we have a properly tagged image, we can use the docker push command to push our image to the Docker Hub registry. $ docker push $DOCKERHUB_USERNAME /python-hello-world:1.0.0 The push refers to a repository [ docker.io/jzaccone/python-hello-world ] 2bce026769ac: Pushed 64d445ecbe93: Pushed 18b27eac38a1: Mounted from library/python 3f6f25cd8b1e: Mounted from library/python b7af9d602a0f: Mounted from library/python ed06208397d5: Mounted from library/python 5accac14015f: Mounted from library/python latest: digest: sha256:508238f264616bf7bf962019d1a3826f8487ed6a48b80bf41fd3996c7175fd0f size: 1786 Check out your image on docker hub in your browser Navigate to Docker Hub and go to your profile to see your newly uploaded image at https://hub.docker.com/repository/docker/<dockerhub-username>/python-hello-world . Now that your image is on Docker Hub, other developers and operations can use the docker pull command to deploy your image to other environments. Note: Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to. We also don't have to go through additional steps to provision these environments. Just one step: install docker, and you are good to go. Run from Docker Hub, docker run -p 5001 :5000 -d $DOCKERHUB_USERNAME /python-hello-world:1.0.0 In a terminal run, curl localhost:5001 which should return hello world! again.","title":"Step 4: Push to a central registry"},{"location":"generatedContent/docker101/lab-2/#step-5-deploying-a-change","text":"The \"hello world!\" application is overrated, let's update the app so that it says \"Hello Beautiful World!\" instead. Update app.py Replace the string \"Hello World\" with \"Hello Beautiful World!\" in app.py . You can update the file with the following command. (copy-paste the entire code block) echo 'from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"hello beautiful world!\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\")' > app.py Rebuild and push your image Now that your app is updated, you need repeat the steps above to rebuild your app and push it to the Docker Hub registry. First rebuild, this time use your Docker Hub username in the build command: $ docker image build -t $DOCKERHUB_USERNAME /python-hello-world . Sending build context to Docker daemon 3 .072kB Step 1 /4 : FROM python:3.6.1-alpine ---> c86415c03c37 Step 2 /4 : RUN pip install flask ---> Using cache ---> ce41f2517c16 Step 3 /4 : CMD python app.py ---> Using cache ---> 0ab91286958b Step 4 /4 : COPY app.py /app.py ---> 3e08b2eeace1 Removing intermediate container 23a955e881fc Successfully built 3e08b2eeace1 Successfully tagged <dockerhub-username>/python-hello-world:latest Notice the \"Using cache\" for steps 1-3. These layers of the Docker Image have already been built and docker image build will use these layers from the cache instead of rebuilding them. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/<dockerhub-username>/python-hello-world ] 94525867566e: Pushed 64d445ecbe93: Layer already exists 18b27eac38a1: Layer already exists 3f6f25cd8b1e: Layer already exists b7af9d602a0f: Layer already exists ed06208397d5: Layer already exists 5accac14015f: Layer already exists latest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786 There is a caching mechanism in place for pushing layers too. Docker Hub already has all but one of the layers from an earlier push, so it only pushes the one layer that has changed. When you change a layer, every layer built on top of that will have to be rebuilt. Each line in a Dockerfile builds a new layer that is built on the layer created from the lines before it. This is why the order of the lines in our Dockerfile is important. We optimized our Dockerfile so that the layer that is most likely to change ( COPY app.py /app.py ) is the last line of the Dockerfile. Generally for an application, your code changes at the most frequent rate. This optimization is particularly important for CI/CD processes, where you want your automation to run as fast as possible.","title":"Step 5: Deploying a Change"},{"location":"generatedContent/docker101/lab-2/#step-6-understanding-image-layers","text":"One of the major design properties of Docker is its use of the union file system. Consider the Dockerfile that we created before: FROM python:3.8-alpine RUN pip install flask CMD [ \"python\" , \"app.py\" ] COPY app.py /app.py Each of these lines is a layer. Each layer contains only the delta, diff or changes from the layers before it. To put these layers together into a single running container, Docker makes use of the union file system to overlay layers transparently into a single view. Each layer of the image is read-only , except for the very top layer which is created for the running container. The read/write container layer implements \"copy-on-write\" which means that files that are stored in lower image layers are pulled up to the read/write container layer only when edits are being made to those files. Those changes are then stored in the running container layer. The \"copy-on-write\" function is very fast, and in almost all cases, does not have a noticeable effect on performance. You can inspect which files have been pulled up to the container level with the docker diff command. More information about how to use docker diff can be found here . Since image layers are read-only , they can be shared by images and by running containers. For instance, creating a new python app with its own Dockerfile with similar base layers, would share all the layers that it had in common with the first python app. FROM python:3.8-alpine RUN pip install flask CMD [ \"python\" , \"app2.py\" ] COPY app2.py /app2.py You can also experience the sharing of layers when you start multiple containers from the same image. Since the containers use the same read-only layers, you can imagine that starting up containers is very fast and has a very low footprint on the host. You may notice that there are duplicate lines in this Dockerfile and the Dockerfile you created earlier in this lab. Although this is a very trivial example, you can pull common lines of both Dockerfiles into a \"base\" Dockerfile, that you can then point to with each of your child Dockerfiles using the FROM command. Image layering enables the docker caching mechanism for builds and pushes. For example, the output for your last docker push shows that some of the layers of your image already exists on the Docker Hub. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/<dockerhub-username>/python-hello-world ] 94525867566e: Pushed 64d445ecbe93: Layer already exists 18b27eac38a1: Layer already exists 3f6f25cd8b1e: Layer already exists b7af9d602a0f: Layer already exists ed06208397d5: Layer already exists 5accac14015f: Layer already exists latest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786 To look more closely at layers, you can use the docker image history command of the python image we created. $ docker image history python-hello-world IMAGE CREATED CREATED BY SIZE COMMENT 3c24958f29d3 17 minutes ago /bin/sh -c #(nop) COPY file:5fef1b9a6220c0e3\u2026 159B d5adbccf5116 17 minutes ago /bin/sh -c #(nop) CMD [\"python\" \"app.py\"] 0B 97d747fc7771 17 minutes ago /bin/sh -c pip install flask 10.7MB 0f03316d4a27 2 weeks ago /bin/sh -c #(nop) CMD [\"python3\"] 0B <missing> 2 weeks ago /bin/sh -c set -ex; wget -O get-pip.py \"$P\u2026 7.24MB <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_GET_PIP_SHA256\u2026 0B <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_GET_PIP_URL=ht\u2026 0B <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_PIP_VERSION=20\u2026 0B <missing> 7 weeks ago /bin/sh -c cd /usr/local/bin && ln -s idle3\u2026 32B <missing> 7 weeks ago /bin/sh -c set -ex && apk add --no-cache --\u2026 29.3MB <missing> 2 months ago /bin/sh -c #(nop) ENV PYTHON_VERSION=3.8.5 0B <missing> 3 months ago /bin/sh -c #(nop) ENV GPG_KEY=E3FF2839C048B\u2026 0B <missing> 3 months ago /bin/sh -c apk add --no-cache ca-certificates 512kB <missing> 3 months ago /bin/sh -c #(nop) ENV LANG=C.UTF-8 0B <missing> 3 months ago /bin/sh -c #(nop) ENV PATH=/usr/local/bin:/\u2026 0B <missing> 3 months ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 3 months ago /bin/sh -c #(nop) ADD file:c92c248239f8c7b9b\u2026 5.57MB Each line represents a layer of the image. You'll notice that the top lines match to your Dockerfile that you created, and the lines below are pulled from the parent python image. Don't worry about the \"\\<missing>\" tags. These are still normal layers; they have just not been given an ID by the docker system.","title":"Step 6: Understanding Image Layers"},{"location":"generatedContent/docker101/lab-2/#step-7-clean-up","text":"Completing this lab results in a bunch of running containers on your host. Let's clean these up. Run docker container stop [container id] for each container that is running First get a list of the containers running using docker container ls . $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0b2ba61df37f python-hello-world \"python app.py\" 7 minutes ago Up 7 minutes 0 .0.0.0:5001->5000/tcp practical_kirch Then run docker container stop [container id] for each container in the list. $ docker container stop 0b2 0b2 Remove the stopped containers docker system prune is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images. $ docker system prune WARNING! This will remove: - all stopped containers - all volumes not used by at least one container - all networks not used by at least one container - all dangling images Are you sure you want to continue ? [ y/N ] y Deleted Containers: 0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26 Total reclaimed space: 300 .3kB","title":"Step 7: Clean up"},{"location":"generatedContent/docker101/lab-2/#summary","text":"In this lab, you started adding value by creating your own custom docker containers. Key Takeaways: The Dockerfile is how you create reproducible builds for your application and how you integrate your application with Docker into the CI/CD pipeline Docker images can be made available to all of your environments through a central registry. The Docker Hub is one example of a registry, but you can deploy your own registry on servers you control. Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to. Docker makes use of the union file system and \"copy on write\" to reuse layers of images. This lowers the footprint of storing images and significantly increases the performance of starting containers. Image layers are cached by the Docker build and push system. No need to rebuild or repush image layers that are already present on the desired system. Each line in a Dockerfile creates a new layer, and because of the layer cache, the lines that change more frequently (e.g. adding source code to an image) should be listed near the bottom of the file.","title":"Summary"},{"location":"generatedContent/docker101/lab-3/","text":"Lab 3 - Managing Data in Containers \u00b6 Overview \u00b6 By default all files created inside a container are stored on a writable container layer. That means that: If the container no longer exists, the data is lost, The container's writable layer is tightly coupled to the host machine, and To manage the file system, you need a storage driver that provides a union file system, using the Linux kernel. This extra abstraction reduces performance compared to data volumes which write directly to the filesystem. Docker provides two options to store files in the host machine: volumes and bind mounts . If you're running Docker on Linux, you can also use a tmpfs mount , and with Docker on Windows you can also use a named pipe . Volumes are stored in the host filesystem that is managed by Docker. Bind mounts are stored anywhere on the host system. tmpfs mounts are stored in the host memory only. Originally, the --mount flag was used for Docker Swarm services and the --volume flag was used for standalone containers. From Docker 17.06 and higher, you can also use --mount for standalone containers and it is in general more explicit and verbose than --volume . Volumes \u00b6 A data volume or volume is a directory that bypasses the Union File System of Docker. There are three types of volumes: anonymous volume, named volume, and host volume. Anonymous Volume \u00b6 Let's create an instance of a popular open source NoSQL database called CouchDB and use an anonymous volume to store the data files for the database. To run an instance of CouchDB, use the CouchDB image from Docker Hub at https://hub.docker.com/_/couchdb . The docs say that the default for CouchDB is to write the database files to disk on the host system using its own internal volume management . Run the following command, docker run -d -p 5984:5984 --name my-couchdb -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 CouchDB will create an anonymous volume and generated a hashed name. Check the volumes on your host system, $ docker volume ls DRIVER VOLUME NAME local f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50 Set an environment variable VOLUME with the value of the generated name, export VOLUME=f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50 And inspect the volume that was created, use the hash name that was generated for the volume, $ docker volume inspect $VOLUME [ { \"CreatedAt\": \"2020-09-24T14:10:07Z\", \"Driver\": \"local\", \"Labels\": null, \"Mountpoint\": \"/var/lib/docker/volumes/f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50/_data\", \"Name\": \"f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50\", \"Options\": null, \"Scope\": \"local\" } ] You see that Docker has created and manages a volume in the Docker host filesystem under /var/lib/docker/volumes/$VOLUME_NAME/_data . Note that this is not a path on the host machine, but a part of the Docker managed filesystem. Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' Stop the container and start the container again, docker stop my-couchdb docker start my-couchdb Retrieve the document in the database to test that the data was persisted, $ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/_all_docs {\"total_rows\":1,\"offset\":0,\"rows\":[ {\"id\":\"1\",\"key\":\"1\",\"value\":{\"rev\":\"1-c09289617e06b96bc747fb1201fea7f1\"}} ]} $ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 {\"_id\":\"1\",\"_rev\":\"1-c09289617e06b96bc747fb1201fea7f1\",\"msg\":\"hello world\"} Sharing Volumes \u00b6 You can share an anonymous volume with another container by using the --volumes-from option. Create a busybox container with an anonymous volume mounted to a directory /data in the container, and using shell commands, write a message to a log file. $ docker run -it --name busybox1 -v /data busybox sh / # echo \"hello from busybox1\" > /data/hi.log / # ls /data hi.log / # exit Make sure the container busybox1 is stopped but not removed. $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 437fb4a271c1 busybox \"sh\" 18 seconds ago Exited (0) 4 seconds ago busybox1 Then create a second busybox container named busybox2 using the --volumes-from option to share the volume created by busybox1 , $ docker run --rm -it --name busybox2 --volumes-from busybox1 busybox sh / # ls -al /data / # cat /data/hi.log hello from busybox1 / # exit Docker created the anynomous volume that you were able to share using the --volumes-from option, and created a new anonymous volume. $ docker volume ls DRIVER VOLUME NAME local 83a3275e889506f3e8ff12cd50f7d5b501c1ace95672334597f9a071df439493 local f4e6b9f9568eeb165a56b2946847035414f5f9c2cad9ff79f18e800277ae1ebd Cleanup the existing volumes and container. docker stop my-couchdb docker rm my-couchdb docker rm busybox1 docker volume rm $(docker volume ls -q) docker system prune -a clear Named Volume \u00b6 A named volume and anonymous volume are similar in that Docker manages where they are located. However, a named volume can be referenced by name when mounting it to a container directory. This is helpful if you want to share a volume across multiple containers. First, create a named volume , docker volume create my-couchdb-data-volume Verify the volume was created, $ docker volume ls DRIVER VOLUME NAME local my-couchdb-data-volume Now create the CouchDB container using the named volume , docker run -d -p 5984:5984 --name my-couchdb -v my-couchdb-data-volume:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 Wait until the CouchDB container is running and the instance is available. Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' It now is easy to share the volume with another container. For instance, read the content of the volume using the busybox image, and share the my-couchdb-data-volume volume by mounting the volume to a directory in the busybox container. $ docker run --rm -it --name busybox -v my-couchdb-data-volume:/myvolume busybox sh / # ls -al /myvolume/ total 40 drwxr-xr-x 4 5984 5984 4096 Sep 24 17:11 . drwxr-xr-x 1 root root 4096 Sep 24 17:14 .. drwxr-xr-x 2 5984 5984 4096 Sep 24 17:11 .delete -rw-r--r-- 1 5984 5984 8388 Sep 24 17:11 _dbs.couch -rw-r--r-- 1 5984 5984 8385 Sep 24 17:11 _nodes.couch drwxr-xr-x 4 5984 5984 4096 Sep 24 17:11 shards / # exit You can check the Docker managed filesystem for volumes by running a busybox container with privileged permission and set the process id to host to inspect the host system, and browse to the Docker managed directories. docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/volumes total 28 -rw------- 1 root root 32768 Nov 10 15:54 metadata.db drwxr-xr-x 3 root root 4096 Nov 10 15:54 my-couchdb-data-volume / # exit Cleanup, docker stop my-couchdb docker rm my-couchdb docker volume rm my-couchdb-data-volume docker system prune -a docker volume prune clear Host Volume \u00b6 When you want to access the volume directory easily from the host machine directly instead of using the Docker managed directories, you can create a host volume . Let's use a directory in the current working directory (indicated with the command pwd ) called data , or choose your own data directory on the host machine, e.g. /home/couchdb/data . We let docker create the $(pwd)/data directory if it does not exist yet. We mount the host volume inside the CouchDB container to the container directory /opt/couchdb/data , which is the default data directory for CouchDB. Run the following command, docker run -d -p 5984:5984 --name my-couchdb -v $(pwd)/data:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 Verify that a directory data was created, $ ls -al total 16 drwxrwsrwx 3 root users 4096 Sep 24 16:27 . drwxrwxr-x 1 root root 4096 Jul 16 20:04 .. drwxr-sr-x 3 5984 5984 4096 Sep 24 16:27 data and that CouchDB has created data files here, $ ls -al data total 32 drwxr-sr-x 3 5984 5984 4096 Sep 24 16:27 . drwxrwsrwx 3 root users 4096 Sep 24 16:27 .. -rw-r--r-- 1 5984 5984 4257 Sep 24 16:27 _dbs.couch drwxr-sr-x 2 5984 5984 4096 Sep 24 16:27 .delete -rw-r--r-- 1 5984 5984 8385 Sep 24 16:27 _nodes.couch Also check that now, no managed volume was created by docker, because we are now using a host volume . docker volume ls and docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/volumes total 24 -rw------- 1 root root 32768 Nov 10 16:00 metadata.db / # exit Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' Note that CouchDB created a folder shards , $ ls -al data total 40 drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 . drwxrwsrwx 3 root users 4096 Sep 24 16:49 .. -rw-r--r-- 1 5984 5984 8388 Sep 24 16:49 _dbs.couch drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 .delete -rw-r--r-- 1 5984 5984 8385 Sep 24 16:49 _nodes.couch drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 shards List the content of the shards directory, $ ls -al data/shards total 16 drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 . drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 .. drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 00000000-7fffffff drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 80000000-ffffffff and the first shard, $ ls -al data/shards/00000000-7fffffff/ total 20 drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 . drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 .. -rw-r--r-- 1 5984 5984 8346 Sep 24 16:49 mydb.1600966173.couch A shard is a horizontal partition of data in a database. Partitioning data into shards and distributing copies of each shard to different nodes in a cluster gives the data greater durability against node loss. CouchDB automatically shards databases and distributes the subsets of documents among nodes. Cleanup, docker stop my-couchdb docker rm my-couchdb sudo rm -rf $(pwd)/data docker system prune -a Bind Mounts \u00b6 The mount syntax is recommended by Docker over the volume syntax. Bind mounts have limited functionality compared to volumes. A file or directory is referenced by its full path on the host machine when mounted into a container. Bind mounts rely on the host machine\u2019s filesystem having a specific directory structure available and you cannot use the Docker CLI to manage bind mounts. Note that bind mounts can change the host filesystem via processes running in a container. Instead of using the -v syntax with three fields separated by colon separator (:), the mount syntax is more verbose and uses multiple key-value pairs: type: bind, volume or tmpfs, source: path to the file or directory on host machine, destination: path in container, readonly, bind-propagation: rprivate, private, rshared, shared, rslave, slave, consistency: consistent, delegated, cached, mount. mkdir data docker run -it --name busybox --mount type=bind,source=\"$(pwd)\"/data,target=/data busybox sh / # echo \"hello busybox\" > /data/hi.txt / # exit cat data/hi.txt [Optional] OverlayFS \u00b6 OverlayFS is a union mount filesystem implementation for Linux. To understand what a Docker volume is, it helps to understand how layers and the filesystem work in Docker. To start a container, Docker takes the read-only image and creates a new read-write layer on top. To view the layers as one, Docker uses a Union File System or OverlayFS (Overlay File System), specifically the overlay2 storage driver. To see Docker host managed files, you need access to the Docker process file system. Using the --privileged and --pid=host flags you can access the host's process ID namespace from inside a container like busybox . You can then browse to Docker's /var/lib/docker/overlay2 directory to see the downloaded layers that are managed by Docker. To view the current list of layers in Docker, $ docker run -it --privileged --pid = host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/overlay2 total 16 drwx------ 3 root root 4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32 drwx------ 5 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d drwx------ 4 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init drwx------ 2 root root 4096 Sep 25 19:44 l / # exit Pull down the ubuntu image and check again, $ docker pull ubuntu Using default tag: latest latest: Pulling from library/ubuntu e6ca3592b144: Pull complete 534a5505201d: Pull complete 990916bd23bb: Pull complete Digest: sha256:cbcf86d7781dbb3a6aa2bcea25403f6b0b443e20b9959165cf52d2cc9608e4b9 Status: Downloaded newer image for ubuntu:latest $ docker run -it --privileged --pid = host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/overlay2/ total 36 drwx------ 3 root root 4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32 drwx------ 4 root root 4096 Sep 25 19:45 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d drwx------ 4 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init drwx------ 4 root root 4096 Sep 25 19:46 a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779 drwx------ 3 root root 4096 Sep 25 19:46 d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65 drwx------ 4 root root 4096 Sep 25 19:46 dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6 drwx------ 5 root root 4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709 drwx------ 4 root root 4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709-init drwx------ 2 root root 4096 Sep 25 19:47 l / # exit You see that pulling down the ubuntu image, implicitly pulled down 4 new layers, a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779 d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65 dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709 The overlay2 storage driver in essence layers different directories on the host and presents them as a single directory. base layer or lowerdir, diff layer or upperdir, overlay layer (user view), and work dir. OverlayFS refers to the lower directories as lowerdir , which contains the base image and the read-only (R/O) layers that are pulled down. The upper directory is called upperdir and is the read-write (R/W) container layer. The unified view or overlay layer is called merged . Finally, a workdir is a required, which is an empty directory used by overlay for internal use. The overlay2 driver supports up to 128 lower OverlayFS layers. The l directory contains shortened layer identifiers as symbolic links. Cleanup, docker system prune -a clear","title":"Lab 3. Manage Data in Containers"},{"location":"generatedContent/docker101/lab-3/#lab-3-managing-data-in-containers","text":"","title":"Lab 3 - Managing Data in Containers"},{"location":"generatedContent/docker101/lab-3/#overview","text":"By default all files created inside a container are stored on a writable container layer. That means that: If the container no longer exists, the data is lost, The container's writable layer is tightly coupled to the host machine, and To manage the file system, you need a storage driver that provides a union file system, using the Linux kernel. This extra abstraction reduces performance compared to data volumes which write directly to the filesystem. Docker provides two options to store files in the host machine: volumes and bind mounts . If you're running Docker on Linux, you can also use a tmpfs mount , and with Docker on Windows you can also use a named pipe . Volumes are stored in the host filesystem that is managed by Docker. Bind mounts are stored anywhere on the host system. tmpfs mounts are stored in the host memory only. Originally, the --mount flag was used for Docker Swarm services and the --volume flag was used for standalone containers. From Docker 17.06 and higher, you can also use --mount for standalone containers and it is in general more explicit and verbose than --volume .","title":"Overview"},{"location":"generatedContent/docker101/lab-3/#volumes","text":"A data volume or volume is a directory that bypasses the Union File System of Docker. There are three types of volumes: anonymous volume, named volume, and host volume.","title":"Volumes"},{"location":"generatedContent/docker101/lab-3/#anonymous-volume","text":"Let's create an instance of a popular open source NoSQL database called CouchDB and use an anonymous volume to store the data files for the database. To run an instance of CouchDB, use the CouchDB image from Docker Hub at https://hub.docker.com/_/couchdb . The docs say that the default for CouchDB is to write the database files to disk on the host system using its own internal volume management . Run the following command, docker run -d -p 5984:5984 --name my-couchdb -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 CouchDB will create an anonymous volume and generated a hashed name. Check the volumes on your host system, $ docker volume ls DRIVER VOLUME NAME local f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50 Set an environment variable VOLUME with the value of the generated name, export VOLUME=f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50 And inspect the volume that was created, use the hash name that was generated for the volume, $ docker volume inspect $VOLUME [ { \"CreatedAt\": \"2020-09-24T14:10:07Z\", \"Driver\": \"local\", \"Labels\": null, \"Mountpoint\": \"/var/lib/docker/volumes/f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50/_data\", \"Name\": \"f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50\", \"Options\": null, \"Scope\": \"local\" } ] You see that Docker has created and manages a volume in the Docker host filesystem under /var/lib/docker/volumes/$VOLUME_NAME/_data . Note that this is not a path on the host machine, but a part of the Docker managed filesystem. Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' Stop the container and start the container again, docker stop my-couchdb docker start my-couchdb Retrieve the document in the database to test that the data was persisted, $ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/_all_docs {\"total_rows\":1,\"offset\":0,\"rows\":[ {\"id\":\"1\",\"key\":\"1\",\"value\":{\"rev\":\"1-c09289617e06b96bc747fb1201fea7f1\"}} ]} $ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 {\"_id\":\"1\",\"_rev\":\"1-c09289617e06b96bc747fb1201fea7f1\",\"msg\":\"hello world\"}","title":"Anonymous Volume"},{"location":"generatedContent/docker101/lab-3/#sharing-volumes","text":"You can share an anonymous volume with another container by using the --volumes-from option. Create a busybox container with an anonymous volume mounted to a directory /data in the container, and using shell commands, write a message to a log file. $ docker run -it --name busybox1 -v /data busybox sh / # echo \"hello from busybox1\" > /data/hi.log / # ls /data hi.log / # exit Make sure the container busybox1 is stopped but not removed. $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 437fb4a271c1 busybox \"sh\" 18 seconds ago Exited (0) 4 seconds ago busybox1 Then create a second busybox container named busybox2 using the --volumes-from option to share the volume created by busybox1 , $ docker run --rm -it --name busybox2 --volumes-from busybox1 busybox sh / # ls -al /data / # cat /data/hi.log hello from busybox1 / # exit Docker created the anynomous volume that you were able to share using the --volumes-from option, and created a new anonymous volume. $ docker volume ls DRIVER VOLUME NAME local 83a3275e889506f3e8ff12cd50f7d5b501c1ace95672334597f9a071df439493 local f4e6b9f9568eeb165a56b2946847035414f5f9c2cad9ff79f18e800277ae1ebd Cleanup the existing volumes and container. docker stop my-couchdb docker rm my-couchdb docker rm busybox1 docker volume rm $(docker volume ls -q) docker system prune -a clear","title":"Sharing Volumes"},{"location":"generatedContent/docker101/lab-3/#named-volume","text":"A named volume and anonymous volume are similar in that Docker manages where they are located. However, a named volume can be referenced by name when mounting it to a container directory. This is helpful if you want to share a volume across multiple containers. First, create a named volume , docker volume create my-couchdb-data-volume Verify the volume was created, $ docker volume ls DRIVER VOLUME NAME local my-couchdb-data-volume Now create the CouchDB container using the named volume , docker run -d -p 5984:5984 --name my-couchdb -v my-couchdb-data-volume:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 Wait until the CouchDB container is running and the instance is available. Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' It now is easy to share the volume with another container. For instance, read the content of the volume using the busybox image, and share the my-couchdb-data-volume volume by mounting the volume to a directory in the busybox container. $ docker run --rm -it --name busybox -v my-couchdb-data-volume:/myvolume busybox sh / # ls -al /myvolume/ total 40 drwxr-xr-x 4 5984 5984 4096 Sep 24 17:11 . drwxr-xr-x 1 root root 4096 Sep 24 17:14 .. drwxr-xr-x 2 5984 5984 4096 Sep 24 17:11 .delete -rw-r--r-- 1 5984 5984 8388 Sep 24 17:11 _dbs.couch -rw-r--r-- 1 5984 5984 8385 Sep 24 17:11 _nodes.couch drwxr-xr-x 4 5984 5984 4096 Sep 24 17:11 shards / # exit You can check the Docker managed filesystem for volumes by running a busybox container with privileged permission and set the process id to host to inspect the host system, and browse to the Docker managed directories. docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/volumes total 28 -rw------- 1 root root 32768 Nov 10 15:54 metadata.db drwxr-xr-x 3 root root 4096 Nov 10 15:54 my-couchdb-data-volume / # exit Cleanup, docker stop my-couchdb docker rm my-couchdb docker volume rm my-couchdb-data-volume docker system prune -a docker volume prune clear","title":"Named Volume"},{"location":"generatedContent/docker101/lab-3/#host-volume","text":"When you want to access the volume directory easily from the host machine directly instead of using the Docker managed directories, you can create a host volume . Let's use a directory in the current working directory (indicated with the command pwd ) called data , or choose your own data directory on the host machine, e.g. /home/couchdb/data . We let docker create the $(pwd)/data directory if it does not exist yet. We mount the host volume inside the CouchDB container to the container directory /opt/couchdb/data , which is the default data directory for CouchDB. Run the following command, docker run -d -p 5984:5984 --name my-couchdb -v $(pwd)/data:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 Verify that a directory data was created, $ ls -al total 16 drwxrwsrwx 3 root users 4096 Sep 24 16:27 . drwxrwxr-x 1 root root 4096 Jul 16 20:04 .. drwxr-sr-x 3 5984 5984 4096 Sep 24 16:27 data and that CouchDB has created data files here, $ ls -al data total 32 drwxr-sr-x 3 5984 5984 4096 Sep 24 16:27 . drwxrwsrwx 3 root users 4096 Sep 24 16:27 .. -rw-r--r-- 1 5984 5984 4257 Sep 24 16:27 _dbs.couch drwxr-sr-x 2 5984 5984 4096 Sep 24 16:27 .delete -rw-r--r-- 1 5984 5984 8385 Sep 24 16:27 _nodes.couch Also check that now, no managed volume was created by docker, because we are now using a host volume . docker volume ls and docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/volumes total 24 -rw------- 1 root root 32768 Nov 10 16:00 metadata.db / # exit Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' Note that CouchDB created a folder shards , $ ls -al data total 40 drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 . drwxrwsrwx 3 root users 4096 Sep 24 16:49 .. -rw-r--r-- 1 5984 5984 8388 Sep 24 16:49 _dbs.couch drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 .delete -rw-r--r-- 1 5984 5984 8385 Sep 24 16:49 _nodes.couch drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 shards List the content of the shards directory, $ ls -al data/shards total 16 drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 . drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 .. drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 00000000-7fffffff drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 80000000-ffffffff and the first shard, $ ls -al data/shards/00000000-7fffffff/ total 20 drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 . drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 .. -rw-r--r-- 1 5984 5984 8346 Sep 24 16:49 mydb.1600966173.couch A shard is a horizontal partition of data in a database. Partitioning data into shards and distributing copies of each shard to different nodes in a cluster gives the data greater durability against node loss. CouchDB automatically shards databases and distributes the subsets of documents among nodes. Cleanup, docker stop my-couchdb docker rm my-couchdb sudo rm -rf $(pwd)/data docker system prune -a","title":"Host Volume"},{"location":"generatedContent/docker101/lab-3/#bind-mounts","text":"The mount syntax is recommended by Docker over the volume syntax. Bind mounts have limited functionality compared to volumes. A file or directory is referenced by its full path on the host machine when mounted into a container. Bind mounts rely on the host machine\u2019s filesystem having a specific directory structure available and you cannot use the Docker CLI to manage bind mounts. Note that bind mounts can change the host filesystem via processes running in a container. Instead of using the -v syntax with three fields separated by colon separator (:), the mount syntax is more verbose and uses multiple key-value pairs: type: bind, volume or tmpfs, source: path to the file or directory on host machine, destination: path in container, readonly, bind-propagation: rprivate, private, rshared, shared, rslave, slave, consistency: consistent, delegated, cached, mount. mkdir data docker run -it --name busybox --mount type=bind,source=\"$(pwd)\"/data,target=/data busybox sh / # echo \"hello busybox\" > /data/hi.txt / # exit cat data/hi.txt","title":"Bind Mounts"},{"location":"generatedContent/docker101/lab-3/#optional-overlayfs","text":"OverlayFS is a union mount filesystem implementation for Linux. To understand what a Docker volume is, it helps to understand how layers and the filesystem work in Docker. To start a container, Docker takes the read-only image and creates a new read-write layer on top. To view the layers as one, Docker uses a Union File System or OverlayFS (Overlay File System), specifically the overlay2 storage driver. To see Docker host managed files, you need access to the Docker process file system. Using the --privileged and --pid=host flags you can access the host's process ID namespace from inside a container like busybox . You can then browse to Docker's /var/lib/docker/overlay2 directory to see the downloaded layers that are managed by Docker. To view the current list of layers in Docker, $ docker run -it --privileged --pid = host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/overlay2 total 16 drwx------ 3 root root 4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32 drwx------ 5 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d drwx------ 4 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init drwx------ 2 root root 4096 Sep 25 19:44 l / # exit Pull down the ubuntu image and check again, $ docker pull ubuntu Using default tag: latest latest: Pulling from library/ubuntu e6ca3592b144: Pull complete 534a5505201d: Pull complete 990916bd23bb: Pull complete Digest: sha256:cbcf86d7781dbb3a6aa2bcea25403f6b0b443e20b9959165cf52d2cc9608e4b9 Status: Downloaded newer image for ubuntu:latest $ docker run -it --privileged --pid = host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/overlay2/ total 36 drwx------ 3 root root 4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32 drwx------ 4 root root 4096 Sep 25 19:45 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d drwx------ 4 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init drwx------ 4 root root 4096 Sep 25 19:46 a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779 drwx------ 3 root root 4096 Sep 25 19:46 d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65 drwx------ 4 root root 4096 Sep 25 19:46 dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6 drwx------ 5 root root 4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709 drwx------ 4 root root 4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709-init drwx------ 2 root root 4096 Sep 25 19:47 l / # exit You see that pulling down the ubuntu image, implicitly pulled down 4 new layers, a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779 d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65 dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709 The overlay2 storage driver in essence layers different directories on the host and presents them as a single directory. base layer or lowerdir, diff layer or upperdir, overlay layer (user view), and work dir. OverlayFS refers to the lower directories as lowerdir , which contains the base image and the read-only (R/O) layers that are pulled down. The upper directory is called upperdir and is the read-write (R/W) container layer. The unified view or overlay layer is called merged . Finally, a workdir is a required, which is an empty directory used by overlay for internal use. The overlay2 driver supports up to 128 lower OverlayFS layers. The l directory contains shortened layer identifiers as symbolic links. Cleanup, docker system prune -a clear","title":"[Optional] OverlayFS"},{"location":"generatedContent/helm101/","text":"Helm 101 \u00b6 Helm is often described as the Kubernetes application package manager. So, what does Helm give you over using kubectl directly? Objectives \u00b6 These labs provide an insight on the advantages of using Helm over using Kubernetes directly through kubectl . In several of the labs there are two scenarios. The first scenario gives an example of how to perform the task using kubectl , the second scenario, using helm . When you complete all the labs, you'll: Understand the core concepts of Helm Understand the advantages of deployment using Helm over Kubernetes directly, looking at: Application management Updates Configuration Revision management Repositories and chart sharing Prerequisites \u00b6 Have a running Kubernetes cluster. See the IBM Cloud Kubernetes Service or Kubernetes Getting Started Guide for details about creating a cluster. Have Helm installed and initialized with the Kubernetes cluster. See Installing Helm on IBM Cloud Kubernetes Service or the Helm Quickstart Guide for getting started with Helm. Helm Overview \u00b6 Helm is a tool that streamlines installation and management of Kubernetes applications. It uses a packaging format called \"charts\", which are a collection of files that describe Kubernetes resources. It can run anywhere (laptop, CI/CD, etc.) and is available for various operating systems, like OSX, Linux and Windows. Helm 3 pivoted from the Helm 2 client-server architecture to a client architecture. The client is still called helm and, there is an improved Go library which encapsulates the Helm logic so that it can be leveraged by different clients. The client is a CLI which users interact with to perform different operations like install/upgrade/delete etc. The client interacts with the Kubernetes API server and the chart repository. It renders Helm template files into Kubernetes manifest files which it uses to perform operations on the Kubernetes cluster via the Kubernetes API. See the Helm Architecture for more details. A chart is organized as a collection of files inside of a directory where the directory name is the name of the chart. It contains template YAML files which facilitates providing configuration values at runtime and eliminates the need of modifying YAML files. These templates provide programming logic as they are based on the Go template language , functions from the Sprig lib and other specialized functions . The chart repository is a location where packaged charts can be stored and shared. This is akin to the image repository in Docker. Refer to The Chart Repository Guide for more details. Helm Abstractions \u00b6 Helm terms: Chart - It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. A chart is basically a package of pre-configured Kubernetes resources. Config - Contains configuration information that can be merged into a packaged chart to create a releasable object. helm - Helm client. It renders charts into manifest files. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. Release - An instance of a chart running in a Kubernetes cluster. Repository - Place where charts reside and can be shared with others. To get started, head on over to Lab 1 .","title":"Helm 101"},{"location":"generatedContent/helm101/#helm-101","text":"Helm is often described as the Kubernetes application package manager. So, what does Helm give you over using kubectl directly?","title":"Helm 101"},{"location":"generatedContent/helm101/#objectives","text":"These labs provide an insight on the advantages of using Helm over using Kubernetes directly through kubectl . In several of the labs there are two scenarios. The first scenario gives an example of how to perform the task using kubectl , the second scenario, using helm . When you complete all the labs, you'll: Understand the core concepts of Helm Understand the advantages of deployment using Helm over Kubernetes directly, looking at: Application management Updates Configuration Revision management Repositories and chart sharing","title":"Objectives"},{"location":"generatedContent/helm101/#prerequisites","text":"Have a running Kubernetes cluster. See the IBM Cloud Kubernetes Service or Kubernetes Getting Started Guide for details about creating a cluster. Have Helm installed and initialized with the Kubernetes cluster. See Installing Helm on IBM Cloud Kubernetes Service or the Helm Quickstart Guide for getting started with Helm.","title":"Prerequisites"},{"location":"generatedContent/helm101/#helm-overview","text":"Helm is a tool that streamlines installation and management of Kubernetes applications. It uses a packaging format called \"charts\", which are a collection of files that describe Kubernetes resources. It can run anywhere (laptop, CI/CD, etc.) and is available for various operating systems, like OSX, Linux and Windows. Helm 3 pivoted from the Helm 2 client-server architecture to a client architecture. The client is still called helm and, there is an improved Go library which encapsulates the Helm logic so that it can be leveraged by different clients. The client is a CLI which users interact with to perform different operations like install/upgrade/delete etc. The client interacts with the Kubernetes API server and the chart repository. It renders Helm template files into Kubernetes manifest files which it uses to perform operations on the Kubernetes cluster via the Kubernetes API. See the Helm Architecture for more details. A chart is organized as a collection of files inside of a directory where the directory name is the name of the chart. It contains template YAML files which facilitates providing configuration values at runtime and eliminates the need of modifying YAML files. These templates provide programming logic as they are based on the Go template language , functions from the Sprig lib and other specialized functions . The chart repository is a location where packaged charts can be stored and shared. This is akin to the image repository in Docker. Refer to The Chart Repository Guide for more details.","title":"Helm Overview"},{"location":"generatedContent/helm101/#helm-abstractions","text":"Helm terms: Chart - It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. A chart is basically a package of pre-configured Kubernetes resources. Config - Contains configuration information that can be merged into a packaged chart to create a releasable object. helm - Helm client. It renders charts into manifest files. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. Release - An instance of a chart running in a Kubernetes cluster. Repository - Place where charts reside and can be shared with others. To get started, head on over to Lab 1 .","title":"Helm Abstractions"},{"location":"generatedContent/helm101/SUMMARY/","text":"Summary \u00b6 Workshop \u00b6 Lab 0 Lab 1 Lab 2 Lab 3 Lab 4 Resources \u00b6 IBM Developer","title":"Summary"},{"location":"generatedContent/helm101/SUMMARY/#summary","text":"","title":"Summary"},{"location":"generatedContent/helm101/SUMMARY/#workshop","text":"Lab 0 Lab 1 Lab 2 Lab 3 Lab 4","title":"Workshop"},{"location":"generatedContent/helm101/SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"generatedContent/helm101/Lab1/","text":"Lab 1. Deploy with Helm \u00b6 Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application. Deploy the application using Helm \u00b6 In this part of the lab, we will deploy the application by using Helm. We will set a release name of guestbook-demo . The Helm chart is available here . Clone the Helm 101 repo to get the files: git clone https://github.com/remkohdev/helm101 A chart is defined as a collection of files that describe a related set of Kubernetes resources. We probably then should take a look at the the files before we go and install the chart. The files for the guestbook chart are as follows: . \u251c\u2500\u2500 Chart.yaml \\\\ A YAML file containing information about the chart \u251c\u2500\u2500 LICENSE \\\\ A plain text file containing the license for the chart \u251c\u2500\u2500 README.md \\\\ A README providing information about the chart usage, configuration, installation etc. \u251c\u2500\u2500 templates \\\\ A directory of templates that will generate valid Kubernetes manifest files when combined with values.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \\\\ Template helpers/definitions that are re-used throughout the chart \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \\\\ Guestbook app container resource \u2502 \u251c\u2500\u2500 guestbook-service.yaml \\\\ Guestbook app service resource \u2502 \u251c\u2500\u2500 NOTES.txt \\\\ A plain text file containing short usage notes about how to access the app post install \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \\\\ Redis master container resource \u2502 \u251c\u2500\u2500 redis-master-service.yaml \\\\ Redis master service resource \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \\\\ Redis slave container resource \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \\\\ Redis slave service resource \u2514\u2500\u2500 values.yaml \\\\ The default configuration values for the chart Note: The template files shown above will be rendered into Kubernetes manifest files before being passed to the Kubernetes API server. Therefore, they map to the manifest files that we deployed when we used kubectl or oc (minus the helper and notes files). Let's go ahead and install the chart now. If the helm-demo namespace does not exist, you will need to create it using: oc new-project helm-demo Install the app as a Helm chart: $ cd helm101/charts $ helm install guestbook-demo ./guestbook/ NAME: guestbook-demo LAST DEPLOYED: Thu Mar 25 20:54:23 2021 NAMESPACE: helm-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace helm-demo' export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 The chart install performs the Kubernetes deployments and service creations of the redis master and slaves, and the guestbook app, as one. This is because the chart is a collection of files that describe a related set of Kubernetes resources and Helm manages the creation of these resources via the Kubernetes API. Check the deployment: oc get deployment guestbook-demo You should see output similar to the following: $ oc get deployment guestbook-demo NAME READY UP-TO-DATE AVAILABLE AGE guestbook-demo 2/2 2 2 51m To check the status of the running application pods, use: oc get pods You should see output similar to the following: $ oc get pods NAME READY STATUS RESTARTS AGE guestbook-demo-6c9cf8b9-jwbs9 1/1 Running 0 52m guestbook-demo-6c9cf8b9-qk4fb 1/1 Running 0 52m redis-master-5d8b66464f-j72jf 1/1 Running 0 52m redis-slave-586b4c847c-2xt99 1/1 Running 0 52m redis-slave-586b4c847c-q7rq5 1/1 Running 0 52m To check the services, use: oc get services $ oc get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook-demo LoadBalancer 172.21.53.13 169.46.30.44 3000:30701/TCP 3m6s redis-master ClusterIP 172.21.242.242 <none> 6379/TCP 3m6s redis-slave ClusterIP 172.21.238.104 <none> 6379/TCP 3m6s Test the guestbook: Locate the external IP and the port of the load balancer by following the \"NOTES\" section in the install output. The commands will be similar to the following: console export SERVICE_IP=$(oc get svc guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') export NODEPORT=$(oc get svc guestbook-demo -o jsonpath='{.spec.ports[0].nodePort}') curl http://$SERVICE_IP:$NODEPORT/rpush/guestbook/hi2 Navigate to the output given (for example http://50.23.5.136:31367 ) in your browser. You should see the guestbook now displaying in your browser: Conclusion \u00b6 Congratulations, you have now deployed an application by using two different methods to Kubernetes! From this lab, you can see that using Helm required less commands and less to think about (by giving it the chart path and not the individual files) versus using kubectl . Helm's application management provides the user with this simplicity. Move on to the next lab, Lab2 , to learn how to update our running app when the chart has been changed.","title":"Lab 1. Deploy with Helm"},{"location":"generatedContent/helm101/Lab1/#lab-1-deploy-with-helm","text":"Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application.","title":"Lab 1. Deploy with Helm"},{"location":"generatedContent/helm101/Lab1/#deploy-the-application-using-helm","text":"In this part of the lab, we will deploy the application by using Helm. We will set a release name of guestbook-demo . The Helm chart is available here . Clone the Helm 101 repo to get the files: git clone https://github.com/remkohdev/helm101 A chart is defined as a collection of files that describe a related set of Kubernetes resources. We probably then should take a look at the the files before we go and install the chart. The files for the guestbook chart are as follows: . \u251c\u2500\u2500 Chart.yaml \\\\ A YAML file containing information about the chart \u251c\u2500\u2500 LICENSE \\\\ A plain text file containing the license for the chart \u251c\u2500\u2500 README.md \\\\ A README providing information about the chart usage, configuration, installation etc. \u251c\u2500\u2500 templates \\\\ A directory of templates that will generate valid Kubernetes manifest files when combined with values.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \\\\ Template helpers/definitions that are re-used throughout the chart \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \\\\ Guestbook app container resource \u2502 \u251c\u2500\u2500 guestbook-service.yaml \\\\ Guestbook app service resource \u2502 \u251c\u2500\u2500 NOTES.txt \\\\ A plain text file containing short usage notes about how to access the app post install \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \\\\ Redis master container resource \u2502 \u251c\u2500\u2500 redis-master-service.yaml \\\\ Redis master service resource \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \\\\ Redis slave container resource \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \\\\ Redis slave service resource \u2514\u2500\u2500 values.yaml \\\\ The default configuration values for the chart Note: The template files shown above will be rendered into Kubernetes manifest files before being passed to the Kubernetes API server. Therefore, they map to the manifest files that we deployed when we used kubectl or oc (minus the helper and notes files). Let's go ahead and install the chart now. If the helm-demo namespace does not exist, you will need to create it using: oc new-project helm-demo Install the app as a Helm chart: $ cd helm101/charts $ helm install guestbook-demo ./guestbook/ NAME: guestbook-demo LAST DEPLOYED: Thu Mar 25 20:54:23 2021 NAMESPACE: helm-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace helm-demo' export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 The chart install performs the Kubernetes deployments and service creations of the redis master and slaves, and the guestbook app, as one. This is because the chart is a collection of files that describe a related set of Kubernetes resources and Helm manages the creation of these resources via the Kubernetes API. Check the deployment: oc get deployment guestbook-demo You should see output similar to the following: $ oc get deployment guestbook-demo NAME READY UP-TO-DATE AVAILABLE AGE guestbook-demo 2/2 2 2 51m To check the status of the running application pods, use: oc get pods You should see output similar to the following: $ oc get pods NAME READY STATUS RESTARTS AGE guestbook-demo-6c9cf8b9-jwbs9 1/1 Running 0 52m guestbook-demo-6c9cf8b9-qk4fb 1/1 Running 0 52m redis-master-5d8b66464f-j72jf 1/1 Running 0 52m redis-slave-586b4c847c-2xt99 1/1 Running 0 52m redis-slave-586b4c847c-q7rq5 1/1 Running 0 52m To check the services, use: oc get services $ oc get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook-demo LoadBalancer 172.21.53.13 169.46.30.44 3000:30701/TCP 3m6s redis-master ClusterIP 172.21.242.242 <none> 6379/TCP 3m6s redis-slave ClusterIP 172.21.238.104 <none> 6379/TCP 3m6s Test the guestbook: Locate the external IP and the port of the load balancer by following the \"NOTES\" section in the install output. The commands will be similar to the following: console export SERVICE_IP=$(oc get svc guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') export NODEPORT=$(oc get svc guestbook-demo -o jsonpath='{.spec.ports[0].nodePort}') curl http://$SERVICE_IP:$NODEPORT/rpush/guestbook/hi2 Navigate to the output given (for example http://50.23.5.136:31367 ) in your browser. You should see the guestbook now displaying in your browser:","title":"Deploy the application using Helm"},{"location":"generatedContent/helm101/Lab1/#conclusion","text":"Congratulations, you have now deployed an application by using two different methods to Kubernetes! From this lab, you can see that using Helm required less commands and less to think about (by giving it the chart path and not the individual files) versus using kubectl . Helm's application management provides the user with this simplicity. Move on to the next lab, Lab2 , to learn how to update our running app when the chart has been changed.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab2/","text":"Lab 2. Make changes with Helm \u00b6 In Lab 1 , we installed the guestbook sample app by using Helm and saw the benefits over using kubectl . In this lab, we're going to look at how to update our running app when the chart has been changed. To demonstrate this, we're going to make changes to the original guestbook chart by: Changing the type from LoadBalancer to NodePort . Scenario 1: Update the application using kubectl \u00b6 In this part of the lab we will update the previously deployed application Guestbook , using Kubernetes directly. Do NOT run the commands , but just review the commands to see how it would be done without Helm. To update a guestbook service from LoadBalancer to NodePort type not using Helm, you need to change the guestbook-service.yaml spec: sed -i.bak 's/LoadBalancer/NodePort/g' guestbook-service.yaml Note: you can reset the files later with a git checkout -- <filename> command, if desired Delete the guestbook service: oc delete svc guestbook Re-create the service with NodePort type: oc create -f guestbook-service.yaml Check the updates, using oc get all $ oc get all NAME READY STATUS RESTARTS AGE pod/guestbook-v1-7fc76dc46-9r4s7 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-hspnk 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-sxzkt 1/1 Running 0 1h pod/redis-master-5d8b66464f-pvbl9 1/1 Running 0 1h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook NodePort 172.21.45.29 <none> 3000:31989/TCP 31s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 9d service/redis-master ClusterIP 172.21.232.61 <none> 6379/TCP 1h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 3/3 3 3 1h deployment.apps/redis-master 1/1 1 1 1h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-7fc76dc46 3 3 3 1h replicaset.apps/redis-master-5d8b66464f 1 1 1 1h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31989 in this output case) to the guestbook service. All redis-slave resources have been removed. View the guestbook Get the public IP of one of your nodes: oc get nodes -o wide Navigate to the IP address plus the node port that printed earlier. Scenario 2: Update the application using Helm \u00b6 In this section, we'll update the previously deployed guestbook-demo application by using Helm. Before we start, let's take a few minutes to see how Helm simplifies the process compared to using Kubernetes directly. Helm's use of a template language provides great flexibility and power to chart authors, which removes the complexity to the chart user. In the guestbook example, we'll use the following capabilities of templating: Values: An object that provides access to the values passed into the chart. An example of this is in guestbook-service , which contains the line type: {{ .Values.service.type }} . This line provides the capability to set the service type during an upgrade or install. Control structures: Also called \u201cactions\u201d in template parlance, control structures provide the template author with the ability to control the flow of a template\u2019s generation. An example of a control structure is in the example below, which contains the line {{- if .Values.redis.slaveEnabled -}} . This line allows us to enable/disable the REDIS master/slave during an upgrade or install. The complete redis-slave-service.yaml file shown below, demonstrates how the file becomes redundant when the slaveEnabled flag is disabled and also how the port value is set. There are more examples of templating functionality in the other chart files. {{ - if .Values.redis.slaveEnabled - }} apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : {{ .Values.redis.port }} targetPort : redis-server selector : app : redis role : slave {{ - end }} Enough talking about the theory. Now let's give it a go! First, lets check the app we deployed in Lab 1 with Helm. This can be done by checking the Helm releases: helm list -n helm-demo Note that we specify the namespace. If not specified, it uses the current namespace context. You should see output similar to the following: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 1 2020-02-24 18:08:02.017401264 +0000 UTC deployed guestbook-0.2.0 The list command provides the list of deployed charts (releases) giving information of chart version, namespace, number of updates (revisions) etc. We now know the release is there from step 1., so we can update the application: $ cd helm101/charts $ helm upgrade guestbook-demo ./guestbook --set service.type = NodePort Release \"guestbook-demo\" has been upgraded. Happy Helming! ... A Helm upgrade takes an existing release and upgrades it according to the information you provide. You should see output similar to the following: $ helm upgrade guestbook-demo ./guestbook --set service.type = NodePort Release \"guestbook-demo\" has been upgraded. Happy Helming! NAME: guestbook-demo LAST DEPLOYED: Thu Mar 25 21:41:01 2021 NAMESPACE: helm-demo STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace helm-demo -o jsonpath=\"{.spec.ports[0].nodePort}\" services guestbook-demo) export NODE_IP=$(kubectl get nodes --namespace helm-demo -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT The upgrade command upgrades the app to a specified version of a chart, updates the app service.type to NodePort . Check the updates, using oc get all : $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn2 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo NodePort 172.21.43.244 <none> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-6c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31202 in this output case) to the guestbook service. When you check the Helm release with helm list , you will see the revision and date has been updated: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 2 2020-02-25 14:23:27.06732381 +0000 UTC deployed guestbook-0.2.0 View the guestbook Get the public IP of one of your nodes: oc get nodes -o wide Set the environment variable for EXTERNAL_IP, EXTERNAL_IP = <node externalip> curl http:// $EXTERNAL_IP : $NODEPORT /rpush/guestbook/hi3 Navigate to the IP address plus the node port that printed earlier. Conclusion \u00b6 Congratulations, you have now updated the applications! Helm does not require any manual changing of resources and is therefore so much easier to upgrade! All configurations can be set on the fly on the command line or by using override files. This is made possible from when the logic was added to the template files, which enables or disables the capability, depending on the flag set. Check out Lab 3 to get an insight into revision management.","title":"Lab 2. Make changes with Helm"},{"location":"generatedContent/helm101/Lab2/#lab-2-make-changes-with-helm","text":"In Lab 1 , we installed the guestbook sample app by using Helm and saw the benefits over using kubectl . In this lab, we're going to look at how to update our running app when the chart has been changed. To demonstrate this, we're going to make changes to the original guestbook chart by: Changing the type from LoadBalancer to NodePort .","title":"Lab 2. Make changes with Helm"},{"location":"generatedContent/helm101/Lab2/#scenario-1-update-the-application-using-kubectl","text":"In this part of the lab we will update the previously deployed application Guestbook , using Kubernetes directly. Do NOT run the commands , but just review the commands to see how it would be done without Helm. To update a guestbook service from LoadBalancer to NodePort type not using Helm, you need to change the guestbook-service.yaml spec: sed -i.bak 's/LoadBalancer/NodePort/g' guestbook-service.yaml Note: you can reset the files later with a git checkout -- <filename> command, if desired Delete the guestbook service: oc delete svc guestbook Re-create the service with NodePort type: oc create -f guestbook-service.yaml Check the updates, using oc get all $ oc get all NAME READY STATUS RESTARTS AGE pod/guestbook-v1-7fc76dc46-9r4s7 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-hspnk 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-sxzkt 1/1 Running 0 1h pod/redis-master-5d8b66464f-pvbl9 1/1 Running 0 1h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook NodePort 172.21.45.29 <none> 3000:31989/TCP 31s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 9d service/redis-master ClusterIP 172.21.232.61 <none> 6379/TCP 1h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 3/3 3 3 1h deployment.apps/redis-master 1/1 1 1 1h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-7fc76dc46 3 3 3 1h replicaset.apps/redis-master-5d8b66464f 1 1 1 1h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31989 in this output case) to the guestbook service. All redis-slave resources have been removed. View the guestbook Get the public IP of one of your nodes: oc get nodes -o wide Navigate to the IP address plus the node port that printed earlier.","title":"Scenario 1: Update the application using kubectl"},{"location":"generatedContent/helm101/Lab2/#scenario-2-update-the-application-using-helm","text":"In this section, we'll update the previously deployed guestbook-demo application by using Helm. Before we start, let's take a few minutes to see how Helm simplifies the process compared to using Kubernetes directly. Helm's use of a template language provides great flexibility and power to chart authors, which removes the complexity to the chart user. In the guestbook example, we'll use the following capabilities of templating: Values: An object that provides access to the values passed into the chart. An example of this is in guestbook-service , which contains the line type: {{ .Values.service.type }} . This line provides the capability to set the service type during an upgrade or install. Control structures: Also called \u201cactions\u201d in template parlance, control structures provide the template author with the ability to control the flow of a template\u2019s generation. An example of a control structure is in the example below, which contains the line {{- if .Values.redis.slaveEnabled -}} . This line allows us to enable/disable the REDIS master/slave during an upgrade or install. The complete redis-slave-service.yaml file shown below, demonstrates how the file becomes redundant when the slaveEnabled flag is disabled and also how the port value is set. There are more examples of templating functionality in the other chart files. {{ - if .Values.redis.slaveEnabled - }} apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : {{ .Values.redis.port }} targetPort : redis-server selector : app : redis role : slave {{ - end }} Enough talking about the theory. Now let's give it a go! First, lets check the app we deployed in Lab 1 with Helm. This can be done by checking the Helm releases: helm list -n helm-demo Note that we specify the namespace. If not specified, it uses the current namespace context. You should see output similar to the following: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 1 2020-02-24 18:08:02.017401264 +0000 UTC deployed guestbook-0.2.0 The list command provides the list of deployed charts (releases) giving information of chart version, namespace, number of updates (revisions) etc. We now know the release is there from step 1., so we can update the application: $ cd helm101/charts $ helm upgrade guestbook-demo ./guestbook --set service.type = NodePort Release \"guestbook-demo\" has been upgraded. Happy Helming! ... A Helm upgrade takes an existing release and upgrades it according to the information you provide. You should see output similar to the following: $ helm upgrade guestbook-demo ./guestbook --set service.type = NodePort Release \"guestbook-demo\" has been upgraded. Happy Helming! NAME: guestbook-demo LAST DEPLOYED: Thu Mar 25 21:41:01 2021 NAMESPACE: helm-demo STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace helm-demo -o jsonpath=\"{.spec.ports[0].nodePort}\" services guestbook-demo) export NODE_IP=$(kubectl get nodes --namespace helm-demo -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT The upgrade command upgrades the app to a specified version of a chart, updates the app service.type to NodePort . Check the updates, using oc get all : $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn2 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo NodePort 172.21.43.244 <none> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-6c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31202 in this output case) to the guestbook service. When you check the Helm release with helm list , you will see the revision and date has been updated: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 2 2020-02-25 14:23:27.06732381 +0000 UTC deployed guestbook-0.2.0 View the guestbook Get the public IP of one of your nodes: oc get nodes -o wide Set the environment variable for EXTERNAL_IP, EXTERNAL_IP = <node externalip> curl http:// $EXTERNAL_IP : $NODEPORT /rpush/guestbook/hi3 Navigate to the IP address plus the node port that printed earlier.","title":"Scenario 2: Update the application using Helm"},{"location":"generatedContent/helm101/Lab2/#conclusion","text":"Congratulations, you have now updated the applications! Helm does not require any manual changing of resources and is therefore so much easier to upgrade! All configurations can be set on the fly on the command line or by using override files. This is made possible from when the logic was added to the template files, which enables or disables the capability, depending on the flag set. Check out Lab 3 to get an insight into revision management.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab3/","text":"Lab 3. Keeping track of the deployed application \u00b6 Let's say you deployed different release versions of your application (i.e., you upgraded the running application). How do you keep track of the versions and how can you do a rollback? Scenario 1: Revision management using Kubernetes \u00b6 In this part of the lab, we should illustrate revision management of guestbook by using Kubernetes directly, but we can't. This is because Kubernetes does not provide any support for revision management. The onus is on you to manage your systems and any updates or changes you make. However, we can use Helm to conduct revision management. Scenario 2: Revision management using Helm \u00b6 In this part of the lab, we illustrate revision management on the deployed application guestbook-demo by using Helm. With Helm, every time an install, upgrade, or rollback happens, the revision number is incremented by 1. The first revision number is always 1. Helm persists release metadata in Secrets (default) or ConfigMaps, stored in the Kubernetes cluster. Every time your release changes, it appends that to the existing data. This provides Helm with the capability to rollback to a previous release. Let's see how this works in practice. Check the number of deployments: helm history guestbook-demo -n helm-demo You should see output similar to the following because we did an upgrade in Lab 2 after the initial install in Lab 1 : $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 deployed guestbook-0.2.0 Upgrade complete Roll back to the previous revision: In this rollback, Helm checks the changes that occured when upgrading from the revision 1 to revision 2. This information enables it to makes the calls to the Kubernetes API server, to update the deployed application as per the initial deployment - in other words with Redis slaves and using a load balancer. Rollback with this command: helm rollback guestbook-demo 1 -n helm-demo $ helm rollback guestbook-demo 1 -n helm-demo Rollback was a success! Happy Helming! Check the history again: helm history guestbook-demo -n helm-demo You should see output similar to the following: $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 superseded guestbook-0.2.0 Upgrade complete 3 Tue Feb 25 14:53:45 2020 deployed guestbook-0.2.0 Rollback to 1 Check the rollback, using: oc get all $ oc get all NAME READY STATUS RESTARTS AGE pod/guestbook-demo-95b976779-g54cs 1/1 Running 0 57m pod/guestbook-demo-95b976779-xvkg2 1/1 Running 0 57m pod/redis-master-7f5c9b648c-kbtvw 1/1 Running 0 57m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo LoadBalancer 172.21.203.237 169.46.30.43 3000:31741/TCP 57m service/redis-master ClusterIP 172.21.14.78 <none> 6379/TCP 57m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 57m deployment.apps/redis-master 1/1 1 1 57m NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-95b976779 2 2 2 57m replicaset.apps/redis-master-7f5c9b648c 1 1 1 57m You can see from the output that the app service is the service type of LoadBalancer again. This shows a complete rollback from the upgrade in Lab 2 Conclusion \u00b6 From this lab, we can say that Helm does revision management well and Kubernetes does not have the capability built in! You might be wondering why we need helm rollback when you could just re-run the helm upgrade from a previous version. And that's a good question. Technically, you should end up with the same resources (with same parameters) deployed. However, the advantage of using helm rollback is that helm manages (ie. remembers) all of the variations/parameters of the previous helm install\\upgrade for you. Doing the rollback via a helm upgrade requires you (and your entire team) to manually track how the command was previously executed. That's not only tedious but very error prone. It is much easier, safer and reliable to let Helm manage all of that for you and all you need to do it tell it which previous version to go back to, and it does the rest. Lab 4 awaits.","title":"Lab 3. Keeping track of the deployed application"},{"location":"generatedContent/helm101/Lab3/#lab-3-keeping-track-of-the-deployed-application","text":"Let's say you deployed different release versions of your application (i.e., you upgraded the running application). How do you keep track of the versions and how can you do a rollback?","title":"Lab 3. Keeping track of the deployed application"},{"location":"generatedContent/helm101/Lab3/#scenario-1-revision-management-using-kubernetes","text":"In this part of the lab, we should illustrate revision management of guestbook by using Kubernetes directly, but we can't. This is because Kubernetes does not provide any support for revision management. The onus is on you to manage your systems and any updates or changes you make. However, we can use Helm to conduct revision management.","title":"Scenario 1: Revision management using Kubernetes"},{"location":"generatedContent/helm101/Lab3/#scenario-2-revision-management-using-helm","text":"In this part of the lab, we illustrate revision management on the deployed application guestbook-demo by using Helm. With Helm, every time an install, upgrade, or rollback happens, the revision number is incremented by 1. The first revision number is always 1. Helm persists release metadata in Secrets (default) or ConfigMaps, stored in the Kubernetes cluster. Every time your release changes, it appends that to the existing data. This provides Helm with the capability to rollback to a previous release. Let's see how this works in practice. Check the number of deployments: helm history guestbook-demo -n helm-demo You should see output similar to the following because we did an upgrade in Lab 2 after the initial install in Lab 1 : $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 deployed guestbook-0.2.0 Upgrade complete Roll back to the previous revision: In this rollback, Helm checks the changes that occured when upgrading from the revision 1 to revision 2. This information enables it to makes the calls to the Kubernetes API server, to update the deployed application as per the initial deployment - in other words with Redis slaves and using a load balancer. Rollback with this command: helm rollback guestbook-demo 1 -n helm-demo $ helm rollback guestbook-demo 1 -n helm-demo Rollback was a success! Happy Helming! Check the history again: helm history guestbook-demo -n helm-demo You should see output similar to the following: $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 superseded guestbook-0.2.0 Upgrade complete 3 Tue Feb 25 14:53:45 2020 deployed guestbook-0.2.0 Rollback to 1 Check the rollback, using: oc get all $ oc get all NAME READY STATUS RESTARTS AGE pod/guestbook-demo-95b976779-g54cs 1/1 Running 0 57m pod/guestbook-demo-95b976779-xvkg2 1/1 Running 0 57m pod/redis-master-7f5c9b648c-kbtvw 1/1 Running 0 57m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo LoadBalancer 172.21.203.237 169.46.30.43 3000:31741/TCP 57m service/redis-master ClusterIP 172.21.14.78 <none> 6379/TCP 57m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 57m deployment.apps/redis-master 1/1 1 1 57m NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-95b976779 2 2 2 57m replicaset.apps/redis-master-7f5c9b648c 1 1 1 57m You can see from the output that the app service is the service type of LoadBalancer again. This shows a complete rollback from the upgrade in Lab 2","title":"Scenario 2: Revision management using Helm"},{"location":"generatedContent/helm101/Lab3/#conclusion","text":"From this lab, we can say that Helm does revision management well and Kubernetes does not have the capability built in! You might be wondering why we need helm rollback when you could just re-run the helm upgrade from a previous version. And that's a good question. Technically, you should end up with the same resources (with same parameters) deployed. However, the advantage of using helm rollback is that helm manages (ie. remembers) all of the variations/parameters of the previous helm install\\upgrade for you. Doing the rollback via a helm upgrade requires you (and your entire team) to manually track how the command was previously executed. That's not only tedious but very error prone. It is much easier, safer and reliable to let Helm manage all of that for you and all you need to do it tell it which previous version to go back to, and it does the rest. Lab 4 awaits.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab4/","text":"Lab 4. Share Helm Charts \u00b6 A key aspect of providing an application means sharing with others. Sharing can be direct counsumption (by users or in CI/CD pipelines) or as a dependency for other charts. If people can't find your app then they can't use it. A means of sharing is a chart repository, which is a location where packaged charts can be stored and shared. As the chart repository only applies to Helm, we will just look at the usage and storage of Helm charts. Using charts from a public repository \u00b6 Helm charts can be available on a remote repository or in a local environment/repository. The remote repositories can be public like Bitnami Charts or IBM Helm Charts , or hosted repositories like on Google Cloud Storage or GitHub. Refer to Helm Chart Repository Guide for more details. You can learn more about the structure of a chart repository by examining the chart index file in this lab. In this part of the lab, we show you how to install the guestbook chart from the Helm101 repo . Check the repositories configured on your system: helm repo list The output should be similar to the following: $ helm repo list Error: no repositories to show Note: Chart repositories are not installed by default with Helm v3. It is expected that you add the repositories for the charts you want to use. The Helm Hub provides a centralized search for publicly available distributed charts. Using the hub you can identify the chart with its hosted repository and then add it to your local respoistory list. The Helm chart repository like Helm v2 is in \"maintenance mode\" and will be deprecated by November 13, 2020. See the project status for more details. Add helm101 repo: helm repo add remkohdev-helm101 https://remkohdev.github.io/helm101/repo/stable/ Should generate an output as follows: $ helm repo add remkohdev-helm101 https://remkohdev.github.io/helm101/repo/stable/ \"remkohdev-helm101\" has been added to your repositories You can also search your repositories for charts by running the following command: helm search repo remkohdev-helm101 $ helm search repo remkohdev-helm101 NAME CHART VERSION APP VERSION DESCRIPTION helm101/guestbook 0.2.0 A Helm chart to deploy Guestbook three tier web. .. Install the chart As mentioned we are going to install the guestbook chart from the Helm101 repo . As the repo is added to our local respoitory list we can reference the chart using the repo name/chart name , in other words helm101/guestbook . To see this in action, you will install the application to a new namespace called repo-demo . If the repo-demo namespace does not exist, create it using: oc create namespace repo-demo Now install the chart using this command: helm install guestbook-demo remkohdev-helm101/guestbook The output should be similar to the following: $ helm install guestbook-demo remkohdev-helm101/guestbook NAME: guestbook-demo LAST DEPLOYED: Thu Mar 25 22:23:34 2021 NAMESPACE: repo-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace repo-demo' export SERVICE_IP=$(kubectl get svc --namespace repo-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 Check that release deployed as expected as follows: $ helm list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo repo-demo 1 2021-03-25 22:23:34.86922709 +0000 UTC deployed guestbook-0.2.0 Conclusion \u00b6 This lab provided you with a brief introduction to the Helm repositories to show how charts can be installed. The ability to share your chart means ease of use to both you and your consumers. Extra \u00b6 You can convert your own Github repository to a Helm Chart repository. Assuming your charts are in charts/guestbook and you want to store the packaged charts into a directory repo/stable , you can package the charts as follows, helm package charts/guestbook -d repo/stable Then index your repo directory, helm repo index --url https://remkohdev.github.io/helm101/repo/stable repo/stable","title":"Lab 4. Share Helm Charts"},{"location":"generatedContent/helm101/Lab4/#lab-4-share-helm-charts","text":"A key aspect of providing an application means sharing with others. Sharing can be direct counsumption (by users or in CI/CD pipelines) or as a dependency for other charts. If people can't find your app then they can't use it. A means of sharing is a chart repository, which is a location where packaged charts can be stored and shared. As the chart repository only applies to Helm, we will just look at the usage and storage of Helm charts.","title":"Lab 4. Share Helm Charts"},{"location":"generatedContent/helm101/Lab4/#using-charts-from-a-public-repository","text":"Helm charts can be available on a remote repository or in a local environment/repository. The remote repositories can be public like Bitnami Charts or IBM Helm Charts , or hosted repositories like on Google Cloud Storage or GitHub. Refer to Helm Chart Repository Guide for more details. You can learn more about the structure of a chart repository by examining the chart index file in this lab. In this part of the lab, we show you how to install the guestbook chart from the Helm101 repo . Check the repositories configured on your system: helm repo list The output should be similar to the following: $ helm repo list Error: no repositories to show Note: Chart repositories are not installed by default with Helm v3. It is expected that you add the repositories for the charts you want to use. The Helm Hub provides a centralized search for publicly available distributed charts. Using the hub you can identify the chart with its hosted repository and then add it to your local respoistory list. The Helm chart repository like Helm v2 is in \"maintenance mode\" and will be deprecated by November 13, 2020. See the project status for more details. Add helm101 repo: helm repo add remkohdev-helm101 https://remkohdev.github.io/helm101/repo/stable/ Should generate an output as follows: $ helm repo add remkohdev-helm101 https://remkohdev.github.io/helm101/repo/stable/ \"remkohdev-helm101\" has been added to your repositories You can also search your repositories for charts by running the following command: helm search repo remkohdev-helm101 $ helm search repo remkohdev-helm101 NAME CHART VERSION APP VERSION DESCRIPTION helm101/guestbook 0.2.0 A Helm chart to deploy Guestbook three tier web. .. Install the chart As mentioned we are going to install the guestbook chart from the Helm101 repo . As the repo is added to our local respoitory list we can reference the chart using the repo name/chart name , in other words helm101/guestbook . To see this in action, you will install the application to a new namespace called repo-demo . If the repo-demo namespace does not exist, create it using: oc create namespace repo-demo Now install the chart using this command: helm install guestbook-demo remkohdev-helm101/guestbook The output should be similar to the following: $ helm install guestbook-demo remkohdev-helm101/guestbook NAME: guestbook-demo LAST DEPLOYED: Thu Mar 25 22:23:34 2021 NAMESPACE: repo-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace repo-demo' export SERVICE_IP=$(kubectl get svc --namespace repo-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 Check that release deployed as expected as follows: $ helm list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo repo-demo 1 2021-03-25 22:23:34.86922709 +0000 UTC deployed guestbook-0.2.0","title":"Using charts from a public repository"},{"location":"generatedContent/helm101/Lab4/#conclusion","text":"This lab provided you with a brief introduction to the Helm repositories to show how charts can be installed. The ability to share your chart means ease of use to both you and your consumers.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab4/#extra","text":"You can convert your own Github repository to a Helm Chart repository. Assuming your charts are in charts/guestbook and you want to store the packaged charts into a directory repo/stable , you can package the charts as follows, helm package charts/guestbook -d repo/stable Then index your repo directory, helm repo index --url https://remkohdev.github.io/helm101/repo/stable repo/stable","title":"Extra"},{"location":"generatedContent/kube101/","text":"IBM Cloud Kubernetes Service Lab \u00b6 An introduction to containers \u00b6 Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers. Objectives \u00b6 This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app Prerequisites \u00b6 A Pay-As-You-Go or Subscription IBM Cloud account Virtual machines \u00b6 Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space. Containers \u00b6 Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources VM vs container \u00b6 Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited. Get set up \u00b6 Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\". Kubernetes and containers: an overview \u00b6 Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started What is Kubernetes? \u00b6 Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster. How was Kubernetes created? \u00b6 Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes . Kubernetes architecture \u00b6 At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully. Kubernetes resource model \u00b6 Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model. Key resources \u00b6 A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store. Kubernetes application deployment workflow \u00b6 User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact. Lab information \u00b6 IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads Lab overview \u00b6 Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"IBM Cloud Kubernetes Service Lab"},{"location":"generatedContent/kube101/#ibm-cloud-kubernetes-service-lab","text":"","title":"IBM Cloud Kubernetes Service Lab"},{"location":"generatedContent/kube101/#an-introduction-to-containers","text":"Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers.","title":"An introduction to containers"},{"location":"generatedContent/kube101/#objectives","text":"This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app","title":"Objectives"},{"location":"generatedContent/kube101/#prerequisites","text":"A Pay-As-You-Go or Subscription IBM Cloud account","title":"Prerequisites"},{"location":"generatedContent/kube101/#virtual-machines","text":"Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space.","title":"Virtual machines"},{"location":"generatedContent/kube101/#containers","text":"Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources","title":"Containers"},{"location":"generatedContent/kube101/#vm-vs-container","text":"Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited.","title":"VM vs container"},{"location":"generatedContent/kube101/#get-set-up","text":"Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\".","title":"Get set up"},{"location":"generatedContent/kube101/#kubernetes-and-containers-an-overview","text":"Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started","title":"Kubernetes and containers: an overview"},{"location":"generatedContent/kube101/#what-is-kubernetes","text":"Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster.","title":"What is Kubernetes?"},{"location":"generatedContent/kube101/#how-was-kubernetes-created","text":"Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes .","title":"How was Kubernetes created?"},{"location":"generatedContent/kube101/#kubernetes-architecture","text":"At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully.","title":"Kubernetes architecture"},{"location":"generatedContent/kube101/#kubernetes-resource-model","text":"Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model.","title":"Kubernetes resource model"},{"location":"generatedContent/kube101/#key-resources","text":"A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store.","title":"Key resources"},{"location":"generatedContent/kube101/#kubernetes-application-deployment-workflow","text":"User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact.","title":"Kubernetes application deployment workflow"},{"location":"generatedContent/kube101/#lab-information","text":"IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads","title":"Lab information"},{"location":"generatedContent/kube101/#lab-overview","text":"Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"Lab overview"},{"location":"generatedContent/kube101/Lab1/","text":"Lab 1. Deploy your first application \u00b6 Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service. 0. Prerequisites \u00b6 IBM Cloud account , Connect to Red Hat OpenShift Kubernetes Service (ROKS) , IBM Cloud Shell , Once the cluster is provisioned, the kubernetes client CLI kubectl needs to be configured to talk to the provisioned cluster. Once your cluster and client is configured, you are ready to deploy your first application, guestbook . 1. Deploy the guestbook application \u00b6 In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . If your current project is the default project, create a new project oc new-project my-guestbook Start by running guestbook : oc create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ oc get pods . You should see output similar to the following: oc get pods Eventually, the status should show up as Running . $ oc get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: oc expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ oc get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed in the <EXTERNAL-IP> column. $ oc get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .189.80.112 Ready master,worker 10d v1.16.2+283af84 10 .189.80.112 169 .60.111.167 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .189.80.113 Ready master,worker 10d v1.16.2+283af84 10 .189.80.113 169 .60.111.164 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 Create an environment variable for the Public IP and the NodePort, NODEPORT = <nodeport> PUBLIC_IP = <EXTERNAL-IP> Now that you have both the address and the port, you can now access the application in the web browser at <EXTERNAL-IP>:<nodeport> . In the example case this is 169.60.111.167:31208 . Test the application, curl http:// $PUBLIC_IP : $NODEPORT /rpush/guestbook/hi1 curl http:// $PUBLIC_IP : $NODEPORT /lrange/guestbook Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"Lab 1. Deploy your first application"},{"location":"generatedContent/kube101/Lab1/#lab-1-deploy-your-first-application","text":"Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service.","title":"Lab 1. Deploy your first application"},{"location":"generatedContent/kube101/Lab1/#0-prerequisites","text":"IBM Cloud account , Connect to Red Hat OpenShift Kubernetes Service (ROKS) , IBM Cloud Shell , Once the cluster is provisioned, the kubernetes client CLI kubectl needs to be configured to talk to the provisioned cluster. Once your cluster and client is configured, you are ready to deploy your first application, guestbook .","title":"0. Prerequisites"},{"location":"generatedContent/kube101/Lab1/#1-deploy-the-guestbook-application","text":"In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . If your current project is the default project, create a new project oc new-project my-guestbook Start by running guestbook : oc create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ oc get pods . You should see output similar to the following: oc get pods Eventually, the status should show up as Running . $ oc get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: oc expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ oc get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed in the <EXTERNAL-IP> column. $ oc get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .189.80.112 Ready master,worker 10d v1.16.2+283af84 10 .189.80.112 169 .60.111.167 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .189.80.113 Ready master,worker 10d v1.16.2+283af84 10 .189.80.113 169 .60.111.164 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 Create an environment variable for the Public IP and the NodePort, NODEPORT = <nodeport> PUBLIC_IP = <EXTERNAL-IP> Now that you have both the address and the port, you can now access the application in the web browser at <EXTERNAL-IP>:<nodeport> . In the example case this is 169.60.111.167:31208 . Test the application, curl http:// $PUBLIC_IP : $NODEPORT /rpush/guestbook/hi1 curl http:// $PUBLIC_IP : $NODEPORT /lrange/guestbook Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"1. Deploy the guestbook application"},{"location":"generatedContent/kube101/Lab2/","text":"Lab 2: Scale and Update Deployments \u00b6 In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: oc create deployment guestbook --image = ibmcom/guestbook:v1 1. Scale apps with replicas \u00b6 A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl or oc provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: oc scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: oc rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ oc rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: oc get pods You should see output listing 10 replicas of your deployment: $ oc get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram: 2. Update and roll back apps \u00b6 Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl or oc , you can now update your deployment to use the v2 image. kubectl or oc allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. oc set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: oc rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ oc rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"EXTERNAL-IP\" use the following commands.: oc describe service guestbook and oc get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: oc rollout undo deployment guestbook You can then use this command to see the status: oc rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ oc get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use oc delete deployment guestbook To remove the service, use: oc delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"Lab 2. Scale and update deployments"},{"location":"generatedContent/kube101/Lab2/#lab-2-scale-and-update-deployments","text":"In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: oc create deployment guestbook --image = ibmcom/guestbook:v1","title":"Lab 2: Scale and Update Deployments"},{"location":"generatedContent/kube101/Lab2/#1-scale-apps-with-replicas","text":"A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl or oc provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: oc scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: oc rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ oc rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: oc get pods You should see output listing 10 replicas of your deployment: $ oc get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram:","title":"1. Scale apps with replicas"},{"location":"generatedContent/kube101/Lab2/#2-update-and-roll-back-apps","text":"Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl or oc , you can now update your deployment to use the v2 image. kubectl or oc allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. oc set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: oc rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ oc rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"EXTERNAL-IP\" use the following commands.: oc describe service guestbook and oc get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: oc rollout undo deployment guestbook You can then use this command to see the status: oc rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ oc get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use oc delete deployment guestbook To remove the service, use: oc delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"2. Update and roll back apps"},{"location":"generatedContent/kube101/Lab3/","text":"Lab 3: Scale and update apps natively, building multi-tier applications. \u00b6 In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the oc command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory. 1. Scale apps natively \u00b6 Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: oc create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. oc get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: oc edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. oc apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: oc create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> Remember, to get the nodeport and EXTERNAL-IP use the following commands. oc describe service guestbook Create an environment variable for the Public IP and the NodePort, NODEPORT = <nodeport> PUBLIC_IP = <EXTERNAL-IP> Now that you have both the address and the port, you can now access the application in the web browser at <EXTERNAL-IP>:<nodeport> . In the example case this is 169.60.111.167:31208 . Test the application, curl http:// $PUBLIC_IP : $NODEPORT /rpush/guestbook/hi2 curl http:// $PUBLIC_IP : $NODEPORT /lrange/guestbook In addition, on OpenShift, you can create a Route to expose your service. A Route is similar to an Ingress object with some additional features, oc expose service guestbook Describe the Route, oc describe route guestbook In consequence, you can access the service now using the host name instead of the External IP. Set an environment variable for host, HOST = <route host name> NODEPORT = <nodeport of the service> Test the application, curl http:// $HOST : $NODEPORT /rpush/guestbook/hi3 curl http:// $HOST : $NODEPORT /lrange/guestbook 2. Connect to a back-end service. \u00b6 If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: oc create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ oc get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. oc exec -it redis-master-q9zg7 redis-cli The oc exec -it redis-master-q9zg7 redis-cli exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the oc exec command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: oc create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: oc delete deploy guestbook-v1 oc create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but if traffic increases, we need to scale the application. Our main bottleneck is that we only have one database server instance to process each request coming though guestbook. One simple High Availability (HA) solution is to separate the reads and write such that they go to different databases, which are replicated properly to achieve data consistency. This model is called primary-secondary (sometimes still referred to as master-slave). The secondary redis talks to the redis primary database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis secondary deployments, which can run several instances to read. The Redis secondary deployment is configured to run two replicas. That's the end of the lab. Now let's clean-up our environment: oc delete -f guestbook-deployment.yaml oc delete -f guestbook-service.yaml oc delete -f redis-master-service.yaml oc delete -f redis-master-deployment.yaml","title":"Lab 3. Build multi-tier applications"},{"location":"generatedContent/kube101/Lab3/#lab-3-scale-and-update-apps-natively-building-multi-tier-applications","text":"In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the oc command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory.","title":"Lab 3: Scale and update apps natively, building multi-tier applications."},{"location":"generatedContent/kube101/Lab3/#1-scale-apps-natively","text":"Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: oc create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. oc get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: oc edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. oc apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: oc create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> Remember, to get the nodeport and EXTERNAL-IP use the following commands. oc describe service guestbook Create an environment variable for the Public IP and the NodePort, NODEPORT = <nodeport> PUBLIC_IP = <EXTERNAL-IP> Now that you have both the address and the port, you can now access the application in the web browser at <EXTERNAL-IP>:<nodeport> . In the example case this is 169.60.111.167:31208 . Test the application, curl http:// $PUBLIC_IP : $NODEPORT /rpush/guestbook/hi2 curl http:// $PUBLIC_IP : $NODEPORT /lrange/guestbook In addition, on OpenShift, you can create a Route to expose your service. A Route is similar to an Ingress object with some additional features, oc expose service guestbook Describe the Route, oc describe route guestbook In consequence, you can access the service now using the host name instead of the External IP. Set an environment variable for host, HOST = <route host name> NODEPORT = <nodeport of the service> Test the application, curl http:// $HOST : $NODEPORT /rpush/guestbook/hi3 curl http:// $HOST : $NODEPORT /lrange/guestbook","title":"1. Scale apps natively"},{"location":"generatedContent/kube101/Lab3/#2-connect-to-a-back-end-service","text":"If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: oc create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ oc get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. oc exec -it redis-master-q9zg7 redis-cli The oc exec -it redis-master-q9zg7 redis-cli exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the oc exec command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: oc create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: oc delete deploy guestbook-v1 oc create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but if traffic increases, we need to scale the application. Our main bottleneck is that we only have one database server instance to process each request coming though guestbook. One simple High Availability (HA) solution is to separate the reads and write such that they go to different databases, which are replicated properly to achieve data consistency. This model is called primary-secondary (sometimes still referred to as master-slave). The secondary redis talks to the redis primary database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis secondary deployments, which can run several instances to read. The Redis secondary deployment is configured to run two replicas. That's the end of the lab. Now let's clean-up our environment: oc delete -f guestbook-deployment.yaml oc delete -f guestbook-service.yaml oc delete -f redis-master-service.yaml oc delete -f redis-master-deployment.yaml","title":"2. Connect to a back-end service."},{"location":"generatedContent/kube101/Lab4/","text":"UNDER CONSTRUCTION \u00b6 1. Check the health of apps \u00b6 Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"***UNDER CONSTRUCTION***"},{"location":"generatedContent/kube101/Lab4/#under-construction","text":"","title":"UNDER CONSTRUCTION"},{"location":"generatedContent/kube101/Lab4/#1-check-the-health-of-apps","text":"Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"1. Check the health of apps"},{"location":"generatedContent/kube101/LabD/","text":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes \u00b6 Advanced debugging techniques to reach your pods. Pod Logs \u00b6 You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached. kubectl edit and vi \u00b6 By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file. busybox pod \u00b6 For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests. Service Endpoints \u00b6 Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service> ImagePullPolicy \u00b6 By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"generatedContent/kube101/LabD/#optional-debugging-lab-tips-and-tricks-for-debugging-applications-in-kubernetes","text":"Advanced debugging techniques to reach your pods.","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"generatedContent/kube101/LabD/#pod-logs","text":"You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached.","title":"Pod Logs"},{"location":"generatedContent/kube101/LabD/#kubectl-edit-and-vi","text":"By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file.","title":"kubectl edit and vi"},{"location":"generatedContent/kube101/LabD/#busybox-pod","text":"For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests.","title":"busybox pod"},{"location":"generatedContent/kube101/LabD/#service-endpoints","text":"Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service>","title":"Service Endpoints"},{"location":"generatedContent/kube101/LabD/#imagepullpolicy","text":"By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"ImagePullPolicy"},{"location":"generatedContent/workshop-setup/","text":"Workshop Setup \u00b6 This repository holds reusable setup instructions for a diverse set of hands-on labs and tutorials. Common Setups \u00b6 Some of the common setup instructions are: New IBM Cloud Upgrade IBM Cloud Account to Pay-As-You-Go Free Kubernetes Cluster Grant Cluster Permissions Connect to RedHat OpenShift Kubernetes Service (ROKS) Labs at CognitiveClass.ai All Setups \u00b6 Calico IBM Cloud Shell Labs at CognitiveClass.ai Free Kubernetes Cluster Grant Cluster Permissions Helm v3 Jenkins MkDocs New IBM Cloud Access OpenShift Cluster at OpenLabs Upgrade IBM Cloud account to Pay-As-You-Go Connect to RedHat OpenShift Kubernetes Service (ROKS) Source-to-Image (S2I)","title":"Workshop Setup"},{"location":"generatedContent/workshop-setup/#workshop-setup","text":"This repository holds reusable setup instructions for a diverse set of hands-on labs and tutorials.","title":"Workshop Setup"},{"location":"generatedContent/workshop-setup/#common-setups","text":"Some of the common setup instructions are: New IBM Cloud Upgrade IBM Cloud Account to Pay-As-You-Go Free Kubernetes Cluster Grant Cluster Permissions Connect to RedHat OpenShift Kubernetes Service (ROKS) Labs at CognitiveClass.ai","title":"Common Setups"},{"location":"generatedContent/workshop-setup/#all-setups","text":"Calico IBM Cloud Shell Labs at CognitiveClass.ai Free Kubernetes Cluster Grant Cluster Permissions Helm v3 Jenkins MkDocs New IBM Cloud Access OpenShift Cluster at OpenLabs Upgrade IBM Cloud account to Pay-As-You-Go Connect to RedHat OpenShift Kubernetes Service (ROKS) Source-to-Image (S2I)","title":"All Setups"},{"location":"generatedContent/workshop-setup/CALICO/","text":"Calico \u00b6 IBM Cloud Kubernetes Service (IKS) \u00b6 Calico is installed by default on IKS. To install the calicoctl client follow the instructions below. Connect to your Kubernetes cluster to start using the calicoctl . If you need help to setup your free IBM Cloud Kubernetes Service (IKS) cluster, go here , or to setup your RedHat OpenShift Kubernetes Service (ROKS), go here . calicoctl \u00b6 The following commands will install the calicoctl client in the browser-based terminal environment at CognitiveClass.ai . If you need help setting up the client terminal at CognitiveClass.ai, follow the instructions here . mkdir calico wget https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl -P ./calico chmod +x calico/calicoctl echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile calicoctl version Client Version: v3.17.1 Git commit: 8871aca3 Unable to retrieve Cluster Version or Type: connection is unauthorized: clusterinformations.crd.projectcalico.org \"default\" is forbidden: User \"system:serviceaccount:sn-labs-remkohdev:remkohdev\" cannot get resource \"clusterinformations\" in API group \"crd.projectcalico.org\" at the cluster scope As kubectl Plugin \u00b6 To install the calicoctl as a plugin to the kubectl client, run the following commands, mkdir calico curl -o ./calico/kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x ./calico/kubectl-calico echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile kubectl calico -h","title":"Calico"},{"location":"generatedContent/workshop-setup/CALICO/#calico","text":"","title":"Calico"},{"location":"generatedContent/workshop-setup/CALICO/#ibm-cloud-kubernetes-service-iks","text":"Calico is installed by default on IKS. To install the calicoctl client follow the instructions below. Connect to your Kubernetes cluster to start using the calicoctl . If you need help to setup your free IBM Cloud Kubernetes Service (IKS) cluster, go here , or to setup your RedHat OpenShift Kubernetes Service (ROKS), go here .","title":"IBM Cloud Kubernetes Service (IKS)"},{"location":"generatedContent/workshop-setup/CALICO/#calicoctl","text":"The following commands will install the calicoctl client in the browser-based terminal environment at CognitiveClass.ai . If you need help setting up the client terminal at CognitiveClass.ai, follow the instructions here . mkdir calico wget https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl -P ./calico chmod +x calico/calicoctl echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile calicoctl version Client Version: v3.17.1 Git commit: 8871aca3 Unable to retrieve Cluster Version or Type: connection is unauthorized: clusterinformations.crd.projectcalico.org \"default\" is forbidden: User \"system:serviceaccount:sn-labs-remkohdev:remkohdev\" cannot get resource \"clusterinformations\" in API group \"crd.projectcalico.org\" at the cluster scope","title":"calicoctl"},{"location":"generatedContent/workshop-setup/CALICO/#as-kubectl-plugin","text":"To install the calicoctl as a plugin to the kubectl client, run the following commands, mkdir calico curl -o ./calico/kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x ./calico/kubectl-calico echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile kubectl calico -h","title":"As kubectl Plugin"},{"location":"generatedContent/workshop-setup/CLOUDSHELL/","text":"IBM Cloud Shell \u00b6 You can access the IBM Cloud Shell directly at IBM Cloud Shell .","title":"IBM Cloud Shell"},{"location":"generatedContent/workshop-setup/CLOUDSHELL/#ibm-cloud-shell","text":"You can access the IBM Cloud Shell directly at IBM Cloud Shell .","title":"IBM Cloud Shell"},{"location":"generatedContent/workshop-setup/COGNITIVECLASS/","text":"CognitiveClass.ai \u00b6 Access CognitiveClass.ai \u00b6 If you have already registered your account, you can access the lab environment at https://labs.cognitiveclass.ai and login. Navigate to https://labs.cognitiveclass.ai/register , Create a new account with your existing IBM Id. Alternative, you can choose to use a Social login (LinkedIn, Google, Github or Facebook), or for using your email account click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger.","title":"CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/COGNITIVECLASS/#cognitiveclassai","text":"","title":"CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/COGNITIVECLASS/#access-cognitiveclassai","text":"If you have already registered your account, you can access the lab environment at https://labs.cognitiveclass.ai and login. Navigate to https://labs.cognitiveclass.ai/register , Create a new account with your existing IBM Id. Alternative, you can choose to use a Social login (LinkedIn, Google, Github or Facebook), or for using your email account click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger.","title":"Access CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/DOCKERHUB/","text":"Docker Hub \u00b6 To push images to a public repository, you can sign up for Docker Hub .","title":"Docker Hub Account"},{"location":"generatedContent/workshop-setup/DOCKERHUB/#docker-hub","text":"To push images to a public repository, you can sign up for Docker Hub .","title":"Docker Hub"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/","text":"Create Free Kubernetes Cluster \u00b6 Prerequirements \u00b6 Free IBM Cloud account, to create a new IBM Cloud account go here . Free IBM Cloud Pay-As-You-Go account, to upgrade to a Pay-As-You-Go account go here IBM Cloud CLI with the Kubernetes Service plugin, see the IBM Cloud CLI Getting Started , or use a pre-installed client environment like the Labs environment at CognitiveClass, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . Using UI \u00b6 Log in to IBM Cloud , Go to the Services Catalog , Filter the services by Kubernetes , Click the Kubernetes Service tile, Or go to the Create Kubernetes Service page directly, Configure the Kubernetes cluster as follows: For Pricing plan select Free . The page options will reload, leaving the default Kubernetes version as your only option, At the time of writing, the default Kubernetes version was set to 1.18.13 , Under Orchestration service , edit the Cluster name to a globally unique name, I recommend to follow a format like username-iks118-1n-cluster1 , where iks118 represents your Kubernetes version, 1n the number of worker nodes, and cluster1 represents your cluster number, in case you have more than 1 cluster. For Resource Group keep the Default resource group, unless you have created a new resource group and want to use your own resource group, Click Create to initiate the create cluster request, You will be forwarded to the Access details for the new cluster, Using CLI \u00b6 To create a free Kubernetes Service, you need to be logged in to a free IBM Cloud Pay-As-You-Go account. IBMID=<your ibm id email> USERNAME=<your short username> KS_CLUSTER_NAME=$USERNAME-iks118-1n-cluster1 KS_ZONE=dal10 KS_VERSION=1.18 KS_FLAVOR=u3c.2x4 KS_WORKERS=1 KS_PROVIDER=classic ibmcloud ks zone ls --provider $KS_PROVIDER ibmcloud ks flavors --zone $KS_ZONE --provider $KS_PROVIDER ibmcloud ks versions ibmcloud login -u $IBMID ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS The response should display similar output as, $ ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS Creating cluster... OK Cluster created with ID bvlntf2d0fe4l9hnres0 Retrieve details of the new cluster, ibmcloud ks cluster get --cluster $KS_CLUSTER_NAME --output json","title":"Create Free Kubernetes Cluster"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/#create-free-kubernetes-cluster","text":"","title":"Create Free Kubernetes Cluster"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/#prerequirements","text":"Free IBM Cloud account, to create a new IBM Cloud account go here . Free IBM Cloud Pay-As-You-Go account, to upgrade to a Pay-As-You-Go account go here IBM Cloud CLI with the Kubernetes Service plugin, see the IBM Cloud CLI Getting Started , or use a pre-installed client environment like the Labs environment at CognitiveClass, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here .","title":"Prerequirements"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/#using-ui","text":"Log in to IBM Cloud , Go to the Services Catalog , Filter the services by Kubernetes , Click the Kubernetes Service tile, Or go to the Create Kubernetes Service page directly, Configure the Kubernetes cluster as follows: For Pricing plan select Free . The page options will reload, leaving the default Kubernetes version as your only option, At the time of writing, the default Kubernetes version was set to 1.18.13 , Under Orchestration service , edit the Cluster name to a globally unique name, I recommend to follow a format like username-iks118-1n-cluster1 , where iks118 represents your Kubernetes version, 1n the number of worker nodes, and cluster1 represents your cluster number, in case you have more than 1 cluster. For Resource Group keep the Default resource group, unless you have created a new resource group and want to use your own resource group, Click Create to initiate the create cluster request, You will be forwarded to the Access details for the new cluster,","title":"Using UI"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/#using-cli","text":"To create a free Kubernetes Service, you need to be logged in to a free IBM Cloud Pay-As-You-Go account. IBMID=<your ibm id email> USERNAME=<your short username> KS_CLUSTER_NAME=$USERNAME-iks118-1n-cluster1 KS_ZONE=dal10 KS_VERSION=1.18 KS_FLAVOR=u3c.2x4 KS_WORKERS=1 KS_PROVIDER=classic ibmcloud ks zone ls --provider $KS_PROVIDER ibmcloud ks flavors --zone $KS_ZONE --provider $KS_PROVIDER ibmcloud ks versions ibmcloud login -u $IBMID ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS The response should display similar output as, $ ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS Creating cluster... OK Cluster created with ID bvlntf2d0fe4l9hnres0 Retrieve details of the new cluster, ibmcloud ks cluster get --cluster $KS_CLUSTER_NAME --output json","title":"Using CLI"},{"location":"generatedContent/workshop-setup/GRANTCLUSTER/","text":"Grant Cluster \u00b6 IBM Kubernetes Service (IKS) and RedHat OpenShift Kubernetes Service (ROKS) \u00b6 The grant cluster method to get access to a Kubernetes cluster will assign access permissions to a cluster or namespace in a cluster that was created prior to the request. Creating a cluster and provisioning the VMs and other resources and deploying the tools may take up to 15 minutes and longer if queued. Permissioning access to an existing cluster in contrast happens in 1 or 2 minutes depending on the number of concurrent requests. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration , Or find instructions to create a new IBM Cloud account here , To grant a cluster, You need to be given a URL to submit your grant cluster request, Open the URL to grant a cluster, e.g. https://<workshop>.mybluemix.net , The grant cluster URL should open the following page, Log in to this IBM Cloud account using the lab key given to you by the instructor and your IBM Id to access your IBM Cloud account, Instructions will ask to Log in to this IBM Cloud account When you click the link to log in to the IBM Cloud account, the IBM Cloud overview page will load with an overview of all resources on the account. In the top right, you will see an active account listed. The active account should be the account on which the cluster is created, which is not your personal account. Click the account dropdown if you need to change the active account. Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, Optionally, you can use the IBM Cloud Shell at https://shell.cloud.ibm.com/ to check access. The cloud shell is attached to your IBM Id. It might take a few moments to create the instance and a new session,","title":"Grant OpenShift Cluster"},{"location":"generatedContent/workshop-setup/GRANTCLUSTER/#grant-cluster","text":"","title":"Grant Cluster"},{"location":"generatedContent/workshop-setup/GRANTCLUSTER/#ibm-kubernetes-service-iks-and-redhat-openshift-kubernetes-service-roks","text":"The grant cluster method to get access to a Kubernetes cluster will assign access permissions to a cluster or namespace in a cluster that was created prior to the request. Creating a cluster and provisioning the VMs and other resources and deploying the tools may take up to 15 minutes and longer if queued. Permissioning access to an existing cluster in contrast happens in 1 or 2 minutes depending on the number of concurrent requests. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration , Or find instructions to create a new IBM Cloud account here , To grant a cluster, You need to be given a URL to submit your grant cluster request, Open the URL to grant a cluster, e.g. https://<workshop>.mybluemix.net , The grant cluster URL should open the following page, Log in to this IBM Cloud account using the lab key given to you by the instructor and your IBM Id to access your IBM Cloud account, Instructions will ask to Log in to this IBM Cloud account When you click the link to log in to the IBM Cloud account, the IBM Cloud overview page will load with an overview of all resources on the account. In the top right, you will see an active account listed. The active account should be the account on which the cluster is created, which is not your personal account. Click the account dropdown if you need to change the active account. Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, Optionally, you can use the IBM Cloud Shell at https://shell.cloud.ibm.com/ to check access. The cloud shell is attached to your IBM Id. It might take a few moments to create the instance and a new session,","title":"IBM Kubernetes Service (IKS) and RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/HELM/","text":"Helm v3 \u00b6 Refer to the Helm install docs for more details. To install Helm v3, run the following commands, In the Cloud Shell , download and unzip Helm v3.2. cd $HOME wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > $HOME/.bash_profile source $HOME/.bash_profile Verify Helm v3 installation. helm version --short outputs, $ helm version --short v3.2.0+ge11b7ce","title":"Helm v3"},{"location":"generatedContent/workshop-setup/HELM/#helm-v3","text":"Refer to the Helm install docs for more details. To install Helm v3, run the following commands, In the Cloud Shell , download and unzip Helm v3.2. cd $HOME wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > $HOME/.bash_profile source $HOME/.bash_profile Verify Helm v3 installation. helm version --short outputs, $ helm version --short v3.2.0+ge11b7ce","title":"Helm v3"},{"location":"generatedContent/workshop-setup/JENKINS/","text":"Jenkins \u00b6 Pre-requirements \u00b6 OpenShift 4.x cluster Setup \u00b6 From the IBM Cloud cluster dashboard, click the OpenShift web console button, First we need to create a new project named jenkins to deploy the Jenkins service to, From the terminal, oc new-project jenkins outputs, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Or in the Openshift web console, go to Home > Projects , Click Create Project For Name enter jenkins , for Display Name enter jenkins , and for Description enter jenkins , Click Create , Go to Operators > OperatorHub , For Filter by keyword enter Jenkins , Select the Jenkins Operator provided by Red Hat , labeled community , Click Continue to Show Community Operator , Review the operator information, and click Install , In the Install Operator window, in the Update Channel section, select alpha under Update Channel , choose A specific namespace in the cluster and in the Installed Namespace section, select the project jenkins from the dropdown, select Automatic under Approval Strategy , Click Install , The Installed Operators page will load, wait until the Jenkins Operator has a Status of Succeeded , Click the installed operator linked Name of Jenkins Operator , In the Provided APIs section, click the Create Instance link in the Jenkins panel, In the Create Jenkins window, select Form View or YAML View for the new Jenkins instance, change the metadata.name to my-jenkins , accept all other specifications, Click Create , Go to Networking > Routes , and look for a new Route jenkins-my-jenkins , Click the link for jenkins-my-jenkins route in the Location column, A route to your Jenkins instance opens in a new browser window or tab, If your page loads with a Application is not available warning, your Jenkins instance is still being deployed and you need to wait a little longer, keep trying until the Jenkins page loads, You can see the progress of the Jenkins startup, by browsing to the Pods of the Deployment of the Jenkins instance that is being created, Click Log in with OpenShift , Click Allow selected permissions , Welcome to Jenkins ! Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save,","title":"Jenkins"},{"location":"generatedContent/workshop-setup/JENKINS/#jenkins","text":"","title":"Jenkins"},{"location":"generatedContent/workshop-setup/JENKINS/#pre-requirements","text":"OpenShift 4.x cluster","title":"Pre-requirements"},{"location":"generatedContent/workshop-setup/JENKINS/#setup","text":"From the IBM Cloud cluster dashboard, click the OpenShift web console button, First we need to create a new project named jenkins to deploy the Jenkins service to, From the terminal, oc new-project jenkins outputs, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Or in the Openshift web console, go to Home > Projects , Click Create Project For Name enter jenkins , for Display Name enter jenkins , and for Description enter jenkins , Click Create , Go to Operators > OperatorHub , For Filter by keyword enter Jenkins , Select the Jenkins Operator provided by Red Hat , labeled community , Click Continue to Show Community Operator , Review the operator information, and click Install , In the Install Operator window, in the Update Channel section, select alpha under Update Channel , choose A specific namespace in the cluster and in the Installed Namespace section, select the project jenkins from the dropdown, select Automatic under Approval Strategy , Click Install , The Installed Operators page will load, wait until the Jenkins Operator has a Status of Succeeded , Click the installed operator linked Name of Jenkins Operator , In the Provided APIs section, click the Create Instance link in the Jenkins panel, In the Create Jenkins window, select Form View or YAML View for the new Jenkins instance, change the metadata.name to my-jenkins , accept all other specifications, Click Create , Go to Networking > Routes , and look for a new Route jenkins-my-jenkins , Click the link for jenkins-my-jenkins route in the Location column, A route to your Jenkins instance opens in a new browser window or tab, If your page loads with a Application is not available warning, your Jenkins instance is still being deployed and you need to wait a little longer, keep trying until the Jenkins page loads, You can see the progress of the Jenkins startup, by browsing to the Pods of the Deployment of the Jenkins instance that is being created, Click Log in with OpenShift , Click Allow selected permissions , Welcome to Jenkins ! Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save,","title":"Setup"},{"location":"generatedContent/workshop-setup/MKDOCS/","text":"MkDocs \u00b6 Installation \u00b6 Go to the Installation instructions for MkDocs, alias python=python3 python --version alias pip=pip3 pip --version pip install --upgrade pip pip install mkdocs mkdocs --version To install Material for MkDocs go to the Getting Started instructions, pip install mkdocs-material In the repository root directory, add a new ~/README.md file, this is the landing page for the Github repository different from the MkDocs landing page. MkDocs assumes its root to be the docs folder. vi README.md Migration \u00b6 Get repository, ORG=<github_org> REPO=<repository> git clone https://github.com/$ORG/$REPO.git cd $REPO When converting an existing repository from Gitbook, rename workshop to docs , mv workshop docs For a repository without Gitbook support, create a new directory docs , mkdir docs When converting from Gitbook, edit .gitbook.yaml and change workshop to docs references, sed -i \"\" 's/workshop/docs/' .gitbook.yaml Skip this step for repositories with a docs/README.md file present! If no file docs/README.md exists, add one. The docs/README.md is the landing page for the MkDocs documentation, cat > docs/README.md <<EOF # $REPO # # About EOF Create a new file .github/workflows/ci.yml to configure a Github action, mkdir -p .github/workflows wget -O .github/workflows/ci.yml https://raw.githubusercontent.com/IBM/workshop-setup/master/.github/workflows/ci.yml In the root folder of your project, create a new file ~/mkdocs.yml and add the following content to the new file, cat > mkdocs.yml <<EOF # Project information site_name: <SITE_NAME> site_url: https://ibm.github.io/<REPO_NAME> site_author: IBM Developer # Repository repo_name: <REPO_NAME> repo_url: https://github.com/ibm/<REPO_NAME> edit_uri: edit/master/docs # Navigation nav: - Welcome: - About the workshop: README.md - Workshop: - Lab 1: lab1.md - Lab 2: lab2.md - References: - Additional resources: references/RESOURCES.md - Contributors: references/CONTRIBUTORS.md # # DO NOT CHANGE BELOW THIS LINE # Copyright copyright: Copyright &copy; 2020 IBM Developer # Theme theme: name: material font: text: IBM Plex Sans code: IBM Plex Mono icon: logo: material/library features: - navigation.tabs # - navigation.instant palette: scheme: default primary: blue accent: blue # Plugins plugins: - search # Customization extra: social: - icon: fontawesome/brands/github link: https://github.com/ibm - icon: fontawesome/brands/twitter link: https://twitter.com/ibmdeveloper - icon: fontawesome/brands/linkedin link: https://www.linkedin.com/company/ibm/ - icon: fontawesome/brands/youtube link: https://www.youtube.com/user/developerworks - icon: fontawesome/brands/dev link: https://dev.to/ibmdeveloper # Extensions markdown_extensions: - abbr - admonition - attr_list - def_list - footnotes - meta - toc: permalink: true - pymdownx.arithmatex: generic: true - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.keys - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences: custom_fences: - name: mermaid class: mermaid format: !!python/name:pymdownx.superfences.fence_code_format - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde EOF Edit the above file mkdocs.yml , change: <SITE_NAME> <REPO_NAME> when converting from Gitbook: copy the navigation links from workshop/SUMMARY.md to the Navigation section and copy-edit the links for the navigation, sed -i \"\" \"s/<SITE_NAME>/$REPO/\" mkdocs.yml sed -i \"\" \"s/<REPO_NAME>/$REPO/\" mkdocs.yml The above navigation example includes two example items, you need to add the files docs/lab1.md and docs/lab2.md to enable the examples, echo '# Lab One' > docs/lab1.md echo '# Lab Two' > docs/lab2.md But for existing content, you need to overwrite the navigation. If your repository has an existing Gitbook, you can copy the navigation in the docs/SUMMARY.md file, to the # Navigation nav: section in mkdocs.yml , Add a file .gitignore with the following exceptions, echo \"\\n# MkDocs\\nsite/\\n\" >> .gitignore Add a new file ~/markdownlint.json with the following rule, cat > markdownlint.json <<EOF { \"line-length\": false } EOF Add a new file .markdownlintignore , when converting from Gitbook, add the following exceptions, cat > .markdownlintignore <<EOF workshop/SUMMARY.md workshop/README.md EOF When migrating from Gitbook , if a .travis.yml file already exists, replace reference to workshop by docs , sed -i \"\" 's/workshop/docs/' .travis.yml Add a new file .travis.yml , cat > .travis.yml <<EOF --- language: node_js node_js: 10 before_script: - npm install markdownlint-cli script: - markdownlint -c .markdownlint.json docs EOF If you don't have a LICENSE file yet, download the LICENSE file, wget -O LICENSE https://raw.githubusercontent.com/IBM/workshop-setup/master/LICENSE Test the docs locally with the command mkdocs serve , Review the WARNING output, fix where necessary, Create a new repository in Github, Initialize the local repository, add all files, commit, add the remote, and push changes, echo \"# test1\" >> README.md git init git add . git commit -m \"first commit\" git branch -M main git remote add origin https://github.com/remkohdev/test1.git git push -u origin main Github pages use a branch called gh-pages , this branch is automatically created if it does not exist by the Github action that is defined in the .github/workflows/ci.yml , Github also needs to resolve the domain name and URI to the repository, and point it to the correct branch of the Github page for the repo. To configure this, in your repo, go to Settings, scroll down to GitHub Pages section and from the Source dropdown, select the branch gh-pages , accept the default /root and click Save . Your MkDocs branch should now be live. Whenever you push changes to your main branch, the Github action to deploy MkDocs will be triggered again, Add an update to your README.md, vi README.md Push changes, git add . git commit -m \"second commit\" git push Check your actions at https://github.com/<ORG>/<REPO>/actions . Export to PDF \u00b6 There are several plugins to export mkdocs to PDF, mkdocs-pdf-export , MkPdf , Install the plugin, Enable the plugin by adding the plugin line to the plugins list in mkdocs.yml , to use the pdf-export plugin, add, plugins: - pdf-export: verbose: true media_type: print enabled_if_env: ENABLE_PDF_EXPORT combined: true Run mkdocs build , which will create a pdf directory with the combined PDF, e.g. to run pdf-export , ENABLE_PDF_EXPORT=1 mkdocs build","title":"MkDocs"},{"location":"generatedContent/workshop-setup/MKDOCS/#mkdocs","text":"","title":"MkDocs"},{"location":"generatedContent/workshop-setup/MKDOCS/#installation","text":"Go to the Installation instructions for MkDocs, alias python=python3 python --version alias pip=pip3 pip --version pip install --upgrade pip pip install mkdocs mkdocs --version To install Material for MkDocs go to the Getting Started instructions, pip install mkdocs-material In the repository root directory, add a new ~/README.md file, this is the landing page for the Github repository different from the MkDocs landing page. MkDocs assumes its root to be the docs folder. vi README.md","title":"Installation"},{"location":"generatedContent/workshop-setup/MKDOCS/#migration","text":"Get repository, ORG=<github_org> REPO=<repository> git clone https://github.com/$ORG/$REPO.git cd $REPO When converting an existing repository from Gitbook, rename workshop to docs , mv workshop docs For a repository without Gitbook support, create a new directory docs , mkdir docs When converting from Gitbook, edit .gitbook.yaml and change workshop to docs references, sed -i \"\" 's/workshop/docs/' .gitbook.yaml Skip this step for repositories with a docs/README.md file present! If no file docs/README.md exists, add one. The docs/README.md is the landing page for the MkDocs documentation, cat > docs/README.md <<EOF # $REPO # # About EOF Create a new file .github/workflows/ci.yml to configure a Github action, mkdir -p .github/workflows wget -O .github/workflows/ci.yml https://raw.githubusercontent.com/IBM/workshop-setup/master/.github/workflows/ci.yml In the root folder of your project, create a new file ~/mkdocs.yml and add the following content to the new file, cat > mkdocs.yml <<EOF # Project information site_name: <SITE_NAME> site_url: https://ibm.github.io/<REPO_NAME> site_author: IBM Developer # Repository repo_name: <REPO_NAME> repo_url: https://github.com/ibm/<REPO_NAME> edit_uri: edit/master/docs # Navigation nav: - Welcome: - About the workshop: README.md - Workshop: - Lab 1: lab1.md - Lab 2: lab2.md - References: - Additional resources: references/RESOURCES.md - Contributors: references/CONTRIBUTORS.md # # DO NOT CHANGE BELOW THIS LINE # Copyright copyright: Copyright &copy; 2020 IBM Developer # Theme theme: name: material font: text: IBM Plex Sans code: IBM Plex Mono icon: logo: material/library features: - navigation.tabs # - navigation.instant palette: scheme: default primary: blue accent: blue # Plugins plugins: - search # Customization extra: social: - icon: fontawesome/brands/github link: https://github.com/ibm - icon: fontawesome/brands/twitter link: https://twitter.com/ibmdeveloper - icon: fontawesome/brands/linkedin link: https://www.linkedin.com/company/ibm/ - icon: fontawesome/brands/youtube link: https://www.youtube.com/user/developerworks - icon: fontawesome/brands/dev link: https://dev.to/ibmdeveloper # Extensions markdown_extensions: - abbr - admonition - attr_list - def_list - footnotes - meta - toc: permalink: true - pymdownx.arithmatex: generic: true - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.keys - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences: custom_fences: - name: mermaid class: mermaid format: !!python/name:pymdownx.superfences.fence_code_format - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde EOF Edit the above file mkdocs.yml , change: <SITE_NAME> <REPO_NAME> when converting from Gitbook: copy the navigation links from workshop/SUMMARY.md to the Navigation section and copy-edit the links for the navigation, sed -i \"\" \"s/<SITE_NAME>/$REPO/\" mkdocs.yml sed -i \"\" \"s/<REPO_NAME>/$REPO/\" mkdocs.yml The above navigation example includes two example items, you need to add the files docs/lab1.md and docs/lab2.md to enable the examples, echo '# Lab One' > docs/lab1.md echo '# Lab Two' > docs/lab2.md But for existing content, you need to overwrite the navigation. If your repository has an existing Gitbook, you can copy the navigation in the docs/SUMMARY.md file, to the # Navigation nav: section in mkdocs.yml , Add a file .gitignore with the following exceptions, echo \"\\n# MkDocs\\nsite/\\n\" >> .gitignore Add a new file ~/markdownlint.json with the following rule, cat > markdownlint.json <<EOF { \"line-length\": false } EOF Add a new file .markdownlintignore , when converting from Gitbook, add the following exceptions, cat > .markdownlintignore <<EOF workshop/SUMMARY.md workshop/README.md EOF When migrating from Gitbook , if a .travis.yml file already exists, replace reference to workshop by docs , sed -i \"\" 's/workshop/docs/' .travis.yml Add a new file .travis.yml , cat > .travis.yml <<EOF --- language: node_js node_js: 10 before_script: - npm install markdownlint-cli script: - markdownlint -c .markdownlint.json docs EOF If you don't have a LICENSE file yet, download the LICENSE file, wget -O LICENSE https://raw.githubusercontent.com/IBM/workshop-setup/master/LICENSE Test the docs locally with the command mkdocs serve , Review the WARNING output, fix where necessary, Create a new repository in Github, Initialize the local repository, add all files, commit, add the remote, and push changes, echo \"# test1\" >> README.md git init git add . git commit -m \"first commit\" git branch -M main git remote add origin https://github.com/remkohdev/test1.git git push -u origin main Github pages use a branch called gh-pages , this branch is automatically created if it does not exist by the Github action that is defined in the .github/workflows/ci.yml , Github also needs to resolve the domain name and URI to the repository, and point it to the correct branch of the Github page for the repo. To configure this, in your repo, go to Settings, scroll down to GitHub Pages section and from the Source dropdown, select the branch gh-pages , accept the default /root and click Save . Your MkDocs branch should now be live. Whenever you push changes to your main branch, the Github action to deploy MkDocs will be triggered again, Add an update to your README.md, vi README.md Push changes, git add . git commit -m \"second commit\" git push Check your actions at https://github.com/<ORG>/<REPO>/actions .","title":"Migration"},{"location":"generatedContent/workshop-setup/MKDOCS/#export-to-pdf","text":"There are several plugins to export mkdocs to PDF, mkdocs-pdf-export , MkPdf , Install the plugin, Enable the plugin by adding the plugin line to the plugins list in mkdocs.yml , to use the pdf-export plugin, add, plugins: - pdf-export: verbose: true media_type: print enabled_if_env: ENABLE_PDF_EXPORT combined: true Run mkdocs build , which will create a pdf directory with the combined PDF, e.g. to run pdf-export , ENABLE_PDF_EXPORT=1 mkdocs build","title":"Export to PDF"},{"location":"generatedContent/workshop-setup/NEWACCOUNT/","text":"Create IBM Cloud ID / Account \u00b6 To create a new account, follow the steps below, Open a web browser and go to the IBM Cloud Sign up page In the Create an account window, enter your company email id and the password you would like to use. Click the Next button. The Verify email section will inform you that a verification code was sent to your email. Switch to your email provider to retrieve the verification code. Then enter the verification code in the Verify email section, and click the Next button. Enter your first name, last name and country in the Personal information section and click the Next button. Click the Create account button. After your account is created, review the IBM Privacy Statement . Then scroll down and click the Proceed button to acknowledge the privacy statement. You are now ready to login to the IBM Cloud. Open a web browser to the IBM Cloud console . If prompted, enter your IBM Id (the email ID you used to create the account above) followed by your password to login. The IBM Cloud dashboard page should load. You have successfully registered a new IBM Cloud account.","title":"IBM Cloud Account"},{"location":"generatedContent/workshop-setup/NEWACCOUNT/#create-ibm-cloud-id-account","text":"To create a new account, follow the steps below, Open a web browser and go to the IBM Cloud Sign up page In the Create an account window, enter your company email id and the password you would like to use. Click the Next button. The Verify email section will inform you that a verification code was sent to your email. Switch to your email provider to retrieve the verification code. Then enter the verification code in the Verify email section, and click the Next button. Enter your first name, last name and country in the Personal information section and click the Next button. Click the Create account button. After your account is created, review the IBM Privacy Statement . Then scroll down and click the Proceed button to acknowledge the privacy statement. You are now ready to login to the IBM Cloud. Open a web browser to the IBM Cloud console . If prompted, enter your IBM Id (the email ID you used to create the account above) followed by your password to login. The IBM Cloud dashboard page should load. You have successfully registered a new IBM Cloud account.","title":"Create IBM Cloud ID / Account"},{"location":"generatedContent/workshop-setup/OPENLABS/","text":"Access OpenShift Cluster at OpenLabs \u00b6 These instructions will walk you through how you can get access to a 2 node OpenShift cluster on IBM Cloud through IBM OpenLabs. This cluster will only be available for 2 hours and then will be deleted. Go to IBM OpenLabs: https://developer.ibm.com/openlabs/openshift Find the lab in the Hands on Labs section called Lab 6: Bring Your Own App You will be asked to sign into IBM Cloud if you aren't already or you can register for a new account. Once you sign in the lab will automatically continue provisioning. You may see messages like Allocating VM and so on. When that is done you should be taken to a new page that has documentation on the left half of the screen along with a terminal environment on the right. If not, click on the Lab 6: Bring Your Own App button again. To access your new OpenShift Cluster, click on the tab on the left side of the page labeled Quick Links and Common Commands From this page you can access the OpenShift Web Console and find instructions on how to log in to the cluster in the terminal with the oc cli tool.","title":"Access OpenShift Cluster at OpenLabs"},{"location":"generatedContent/workshop-setup/OPENLABS/#access-openshift-cluster-at-openlabs","text":"These instructions will walk you through how you can get access to a 2 node OpenShift cluster on IBM Cloud through IBM OpenLabs. This cluster will only be available for 2 hours and then will be deleted. Go to IBM OpenLabs: https://developer.ibm.com/openlabs/openshift Find the lab in the Hands on Labs section called Lab 6: Bring Your Own App You will be asked to sign into IBM Cloud if you aren't already or you can register for a new account. Once you sign in the lab will automatically continue provisioning. You may see messages like Allocating VM and so on. When that is done you should be taken to a new page that has documentation on the left half of the screen along with a terminal environment on the right. If not, click on the Lab 6: Bring Your Own App button again. To access your new OpenShift Cluster, click on the tab on the left side of the page labeled Quick Links and Common Commands From this page you can access the OpenShift Web Console and find instructions on how to log in to the cluster in the terminal with the oc cli tool.","title":"Access OpenShift Cluster at OpenLabs"},{"location":"generatedContent/workshop-setup/PAYASYOUGO/","text":"Upgrade to Pay-As-You-Go Account \u00b6 Login to your IBM Cloud account , Go to account settings , Click the Add credit card button, to create the Pay-As-You-Go account, which will unlock the full catalog, Select the Account type , choose between Company and Personal . For the setup, I will choose Personal . Click Next , Enter your Billing Information , click Next , Enter your Payment information , click Next , Acknowledge that your card will not be charged, but a $1.00 hold will be placed while authorizing the card, Click Upgrade Account , You will receive $200 credit, in case you want to use paid services, Click Done , Under Account settings you should now see Account Type as Pay-As-You-Go . Go to the IBM Cloud Catalog and filter Free and Lite services . Free and Lite Services \u00b6 At the time of writing this setup 115 free and lite services were listed, among other: Analytics Engine, API Connect, API Gateway, App Connect, App ID, Auto Scale for VPC, Certificate Manager, Cloudant, Code Engine, Container Registry, Continuous Delivery, DB2, Direct Link Connect, Discovery, Event Streams with Kafka, Hyper Protect, LogDNA, Sysdig, IBM Cognos, Internet of Things Platform, Internet Services with CloudFlare, Key Protect, Knowledge Studio, Kubernetes Service, Language Translator, Load Balancer for VPC, Machine Learning, MQ, Natural Language Classifier, Natural Language Understanding, Object Storage, PagerDuty, Schematics with Terraform, Secrets Manager, Secure Gateway, Speech to Text, SQL Query, Object Storage with ANSI SQL, Streaming Analytics, Text to Speech, Tone Analyzer, Toolchain, Twilio, Visual Recognition, Voice Agent with Watson, Watson Assistant, Watson Knowledge Catalog, Watson OpenScale, Watson Studio,","title":"Upgrade to Pay-As-You-Go Account"},{"location":"generatedContent/workshop-setup/PAYASYOUGO/#upgrade-to-pay-as-you-go-account","text":"Login to your IBM Cloud account , Go to account settings , Click the Add credit card button, to create the Pay-As-You-Go account, which will unlock the full catalog, Select the Account type , choose between Company and Personal . For the setup, I will choose Personal . Click Next , Enter your Billing Information , click Next , Enter your Payment information , click Next , Acknowledge that your card will not be charged, but a $1.00 hold will be placed while authorizing the card, Click Upgrade Account , You will receive $200 credit, in case you want to use paid services, Click Done , Under Account settings you should now see Account Type as Pay-As-You-Go . Go to the IBM Cloud Catalog and filter Free and Lite services .","title":"Upgrade to Pay-As-You-Go Account"},{"location":"generatedContent/workshop-setup/PAYASYOUGO/#free-and-lite-services","text":"At the time of writing this setup 115 free and lite services were listed, among other: Analytics Engine, API Connect, API Gateway, App Connect, App ID, Auto Scale for VPC, Certificate Manager, Cloudant, Code Engine, Container Registry, Continuous Delivery, DB2, Direct Link Connect, Discovery, Event Streams with Kafka, Hyper Protect, LogDNA, Sysdig, IBM Cognos, Internet of Things Platform, Internet Services with CloudFlare, Key Protect, Knowledge Studio, Kubernetes Service, Language Translator, Load Balancer for VPC, Machine Learning, MQ, Natural Language Classifier, Natural Language Understanding, Object Storage, PagerDuty, Schematics with Terraform, Secrets Manager, Secure Gateway, Speech to Text, SQL Query, Object Storage with ANSI SQL, Streaming Analytics, Text to Speech, Tone Analyzer, Toolchain, Twilio, Visual Recognition, Voice Agent with Watson, Watson Assistant, Watson Knowledge Catalog, Watson OpenScale, Watson Studio,","title":"Free and Lite Services"},{"location":"generatedContent/workshop-setup/PLAYWITHDOCKER/","text":"Play-with-Docker \u00b6 To use the client terminal with Docker daemon, you can sign in to Play with Docker using a Docker account.","title":"Play-with-Docker Shell"},{"location":"generatedContent/workshop-setup/PLAYWITHDOCKER/#play-with-docker","text":"To use the client terminal with Docker daemon, you can sign in to Play with Docker using a Docker account.","title":"Play-with-Docker"},{"location":"generatedContent/workshop-setup/ROKS/","text":"Connect to RedHat OpenShift Kubernetes Service (ROKS) \u00b6 Login to IBM Cloud \u00b6 To login to IBM Cloud, Go to https://cloud.ibm.com in your browser and login. Make sure that you are in the correct account#. Note: you may not have access to your OpenShift cluster if you are not in the right account#. Shell \u00b6 Most of the labs are run using CLI commands. The IBM Cloud Shell available at https://shell.cloud.ibm.com is preconfigured with the full IBM Cloud CLI and tons of plug-ins and tools that you can use to manage apps, resources, and infrastructure. Login to OpenShift \u00b6 In a new browser tab, go to https://cloud.ibm.com/kubernetes/clusters?platformType=openshift . Make sure the account holding the cluster is selected, Select your cluster instance and open it. Click OpenShift web console button on the top. Click on your username in the upper right and select Copy Login Command option. Click the Display Token link. Copy the contents of the field Log in with this token to the clipboard. It provides a login command with a valid token for your username. Go to the your shell terminal. Paste the oc login command in the IBM Cloud Shell terminal and run it. Verify you connect to the right cluster. oc get all oc get nodes -o wide Optionally, for convenience, set an environment variable for your cluster name. export CLUSTER_NAME=<your_cluster_name>","title":"Connect to ROKS"},{"location":"generatedContent/workshop-setup/ROKS/#connect-to-redhat-openshift-kubernetes-service-roks","text":"","title":"Connect to RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/ROKS/#login-to-ibm-cloud","text":"To login to IBM Cloud, Go to https://cloud.ibm.com in your browser and login. Make sure that you are in the correct account#. Note: you may not have access to your OpenShift cluster if you are not in the right account#.","title":"Login to IBM Cloud"},{"location":"generatedContent/workshop-setup/ROKS/#shell","text":"Most of the labs are run using CLI commands. The IBM Cloud Shell available at https://shell.cloud.ibm.com is preconfigured with the full IBM Cloud CLI and tons of plug-ins and tools that you can use to manage apps, resources, and infrastructure.","title":"Shell"},{"location":"generatedContent/workshop-setup/ROKS/#login-to-openshift","text":"In a new browser tab, go to https://cloud.ibm.com/kubernetes/clusters?platformType=openshift . Make sure the account holding the cluster is selected, Select your cluster instance and open it. Click OpenShift web console button on the top. Click on your username in the upper right and select Copy Login Command option. Click the Display Token link. Copy the contents of the field Log in with this token to the clipboard. It provides a login command with a valid token for your username. Go to the your shell terminal. Paste the oc login command in the IBM Cloud Shell terminal and run it. Verify you connect to the right cluster. oc get all oc get nodes -o wide Optionally, for convenience, set an environment variable for your cluster name. export CLUSTER_NAME=<your_cluster_name>","title":"Login to OpenShift"},{"location":"generatedContent/workshop-setup/S2I/","text":"Source-to-Image (S2I) \u00b6 To install s2i CLI tool, Download tar file. curl -s https://api.github.com/repos/openshift/source-to-image/releases/latest \\ | grep browser_download_url \\ | grep linux-amd64 \\ | cut -d '\"' -f 4 \\ | wget -qi - Unzip tar file sudo tar xvf source-to-image*.gz Make s2i CLI accessiblee. sudo mv s2i /usr/local/bin verify s2i version With that done, you can start the lab.","title":"Source-to-Image (S2I)"},{"location":"generatedContent/workshop-setup/S2I/#source-to-image-s2i","text":"To install s2i CLI tool, Download tar file. curl -s https://api.github.com/repos/openshift/source-to-image/releases/latest \\ | grep browser_download_url \\ | grep linux-amd64 \\ | cut -d '\"' -f 4 \\ | wget -qi - Unzip tar file sudo tar xvf source-to-image*.gz Make s2i CLI accessiblee. sudo mv s2i /usr/local/bin verify s2i version With that done, you can start the lab.","title":"Source-to-Image (S2I)"},{"location":"generatedContent/workshop-setup/VAULT/","text":"Vault \u00b6 Vault CLI \u00b6 See Vault Project Install Vault , or Hashicorp Install Vault . OSX \u00b6 Homebrew formula for Vault . brew install vault $ vault version Vault v1.6.3 ( 'b540be4b7ec48d0dd7512c8d8df9399d6bf84d76+CHANGES' ) Linux \u00b6 wget https://releases.hashicorp.com/vault/1.6.3/vault_1.6.3_linux_amd64.zip -P ./vault-1.6.3 cd vault-1.6.3 unzip vault_1.6.3_linux_amd64.zip cd .. chmod +x vault-1.6.3/vault echo \"export PATH= $( pwd ) /vault-1.6.3/: $PATH \" > $HOME /.bash_profile source $HOME /.bash_profile vault version","title":"Vault"},{"location":"generatedContent/workshop-setup/VAULT/#vault","text":"","title":"Vault"},{"location":"generatedContent/workshop-setup/VAULT/#vault-cli","text":"See Vault Project Install Vault , or Hashicorp Install Vault .","title":"Vault CLI"},{"location":"generatedContent/workshop-setup/VAULT/#osx","text":"Homebrew formula for Vault . brew install vault $ vault version Vault v1.6.3 ( 'b540be4b7ec48d0dd7512c8d8df9399d6bf84d76+CHANGES' )","title":"OSX"},{"location":"generatedContent/workshop-setup/VAULT/#linux","text":"wget https://releases.hashicorp.com/vault/1.6.3/vault_1.6.3_linux_amd64.zip -P ./vault-1.6.3 cd vault-1.6.3 unzip vault_1.6.3_linux_amd64.zip cd .. chmod +x vault-1.6.3/vault echo \"export PATH= $( pwd ) /vault-1.6.3/: $PATH \" > $HOME /.bash_profile source $HOME /.bash_profile vault version","title":"Linux"}]}